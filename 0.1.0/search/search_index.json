{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tavros Tavros is a cost-effective, cloud-native, and modular integration platform composed of best-of-breed, and seamlessly integrated open-source components. Ansible Collection - ms3_inc.tavros The objective of this Ansible Collection is to provide the necessary Ansible Playbooks to configure, provision, and manage the Tavros Kubernetes Cluster and supported components. Provision Playbook The provision playbook provisions a Kubernetes cluster and configures Tavros's platform components, application environments, etc. All of the components are configurable through Ansible variables or the default configuration can be chosen. See the provision playbook's documentation for more information. Supported Platform Components Concern Component Version Platform GitOps Flux v2 0.10.0 Platform GitOps Sealed Secrets 0.15.0 API Gateway and Manager Kong 2.3.3 API Portal Kong Enterprise Edition 2.3.3 Service Mesh Kuma 1.2.0 Identity and Access Management Keycloak 12.0.4 Artifact Management Nexus Repository Manager 3.28.1 Continuous Delivery Jenkins 2.277.4 Observability Elastic Cloud 7.13.4 Observability Jaeger 1.22.0 Static Code Qualitative Analysis Sonarqube 8.5 Roadmap The Tavros team will maintain an up to date roadmap for major and minor releases through its Milestones . For items that are not yet targeting a milestone, you can see our Backlog Architectural Decision Log This project documents significant architectural decisions in MADR, a lightweight format for recording architectural decisions in Markdown. See our Architectural Decision Log .","title":"Home"},{"location":"#tavros","text":"Tavros is a cost-effective, cloud-native, and modular integration platform composed of best-of-breed, and seamlessly integrated open-source components.","title":"Tavros"},{"location":"#ansible-collection-ms3_inctavros","text":"The objective of this Ansible Collection is to provide the necessary Ansible Playbooks to configure, provision, and manage the Tavros Kubernetes Cluster and supported components.","title":"Ansible Collection - ms3_inc.tavros"},{"location":"#provision-playbook","text":"The provision playbook provisions a Kubernetes cluster and configures Tavros's platform components, application environments, etc. All of the components are configurable through Ansible variables or the default configuration can be chosen. See the provision playbook's documentation for more information.","title":"Provision Playbook"},{"location":"#supported-platform-components","text":"Concern Component Version Platform GitOps Flux v2 0.10.0 Platform GitOps Sealed Secrets 0.15.0 API Gateway and Manager Kong 2.3.3 API Portal Kong Enterprise Edition 2.3.3 Service Mesh Kuma 1.2.0 Identity and Access Management Keycloak 12.0.4 Artifact Management Nexus Repository Manager 3.28.1 Continuous Delivery Jenkins 2.277.4 Observability Elastic Cloud 7.13.4 Observability Jaeger 1.22.0 Static Code Qualitative Analysis Sonarqube 8.5","title":"Supported Platform Components"},{"location":"#roadmap","text":"The Tavros team will maintain an up to date roadmap for major and minor releases through its Milestones . For items that are not yet targeting a milestone, you can see our Backlog","title":"Roadmap"},{"location":"#architectural-decision-log","text":"This project documents significant architectural decisions in MADR, a lightweight format for recording architectural decisions in Markdown. See our Architectural Decision Log .","title":"Architectural Decision Log"},{"location":"adr/","text":"Architectural Decision Log This log lists the architectural decisions for Tavros. ADR-0000 - Prefer Proven FOSS Components with Optional Support for Licensed Derivatives ADR-0001 - Apache Camel as the Default Integration Framework ADR-0002 - Spring Boot as the Base Application Framework ADR-0003 - DataSonnet as the Default Data Transformation Language ADR-0004 - OpenTracing for In-Process Tracing API ADR-0005 - Kubernetes as the Computing Platform ADR-0006 - Kops to Provision a Kubernetes Cluster ADR-0007 - Flux to Provide Platform GitOps ADR-0008 - Kubeseal to Securely Manage Secrets in GitOps ADR-0009 - Keycloak for Indetity and Access Management ADR-0010 - Kong as Kubernetes Ingress and API gateway ADR-0011 - PostgreSQL as the Platform's Default Database ADR-0012 - Gitea for a Lightweight Git Server ADR-0013 - Kuma for Service Mesh ADR-0014 - Jenkins for Continuous Integration ADR-0015 - Sonarqube for Application Static Code Analysis ADR-0016 - Elastic Cloud for Observability Data Aggregation and Visualization ADR-0017 - Jaeger for Tracing with Elasticsearch Backend ADR-0018 - Nexus Repository Manager for Artifact Management ADR-0019 - Prefer Daemonsets Over Sidecars ADR-0020 - Spring Cloud Config for Application Configuration Management ADR-0021 - Prefer Kong Enterprise Edition ADR-0022 - Use Ansible as the Provisioning Engine ADR-0023 - Helm and Operators for Component Installation and Management ADR-0024 - Use Ansible Collection to Structure and Package Ansible Code ADR-0025 - Setup Sandbox and Production Kuma Meshes and Kong Ingress Controllers ADR-0026 - Setup Sandbox and Production Keycloak Realms ADR-0027 - cert-manager for Certificate Management ADR-0028 - Use Markdown Architectural Decision Records ADR-0029 - Tavros as a Single Tenant Platform For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/ . General information about architectural decision records is available at https://adr.github.io/ .","title":"Architectural Decision Log"},{"location":"adr/#architectural-decision-log","text":"This log lists the architectural decisions for Tavros. ADR-0000 - Prefer Proven FOSS Components with Optional Support for Licensed Derivatives ADR-0001 - Apache Camel as the Default Integration Framework ADR-0002 - Spring Boot as the Base Application Framework ADR-0003 - DataSonnet as the Default Data Transformation Language ADR-0004 - OpenTracing for In-Process Tracing API ADR-0005 - Kubernetes as the Computing Platform ADR-0006 - Kops to Provision a Kubernetes Cluster ADR-0007 - Flux to Provide Platform GitOps ADR-0008 - Kubeseal to Securely Manage Secrets in GitOps ADR-0009 - Keycloak for Indetity and Access Management ADR-0010 - Kong as Kubernetes Ingress and API gateway ADR-0011 - PostgreSQL as the Platform's Default Database ADR-0012 - Gitea for a Lightweight Git Server ADR-0013 - Kuma for Service Mesh ADR-0014 - Jenkins for Continuous Integration ADR-0015 - Sonarqube for Application Static Code Analysis ADR-0016 - Elastic Cloud for Observability Data Aggregation and Visualization ADR-0017 - Jaeger for Tracing with Elasticsearch Backend ADR-0018 - Nexus Repository Manager for Artifact Management ADR-0019 - Prefer Daemonsets Over Sidecars ADR-0020 - Spring Cloud Config for Application Configuration Management ADR-0021 - Prefer Kong Enterprise Edition ADR-0022 - Use Ansible as the Provisioning Engine ADR-0023 - Helm and Operators for Component Installation and Management ADR-0024 - Use Ansible Collection to Structure and Package Ansible Code ADR-0025 - Setup Sandbox and Production Kuma Meshes and Kong Ingress Controllers ADR-0026 - Setup Sandbox and Production Keycloak Realms ADR-0027 - cert-manager for Certificate Management ADR-0028 - Use Markdown Architectural Decision Records ADR-0029 - Tavros as a Single Tenant Platform For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/ . General information about architectural decision records is available at https://adr.github.io/ .","title":"Architectural Decision Log"},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/","text":"Prefer Proven FOSS Components with Optional Support for Licensed Derivatives Status: accepted Deciders: @k2merlinsix, @jam01 Date: 2020-08 Context and Problem Statement Should we include licensed or non-FOSS components in the Tavros platform for a given component or component's feature? Decision Drivers Total cost of ownership Providing the necessary features for a competitive product Decision Outcome We will prioritize FOSS components, while providing optional support for paid or licensed derivatives, e.g.: Elastic Stack Open Source vs Enterprise subscription, Apache Camel vs JBoss Fuse, etc. Ultimately total cost of ownership will be the deciding factor for our customers when comparing to established alternatives. There are many FOSS components that address different concerns that we can incorporate and still provide a best-in-class platform. Negative Consequences High number of components to configure and integrate","title":"Prefer Proven FOSS Components with Optional Support for Licensed Derivatives"},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/#prefer-proven-foss-components-with-optional-support-for-licensed-derivatives","text":"Status: accepted Deciders: @k2merlinsix, @jam01 Date: 2020-08","title":"Prefer Proven FOSS Components with Optional Support for Licensed Derivatives"},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/#context-and-problem-statement","text":"Should we include licensed or non-FOSS components in the Tavros platform for a given component or component's feature?","title":"Context and Problem Statement"},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/#decision-drivers","text":"Total cost of ownership Providing the necessary features for a competitive product","title":"Decision Drivers "},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/#decision-outcome","text":"We will prioritize FOSS components, while providing optional support for paid or licensed derivatives, e.g.: Elastic Stack Open Source vs Enterprise subscription, Apache Camel vs JBoss Fuse, etc. Ultimately total cost of ownership will be the deciding factor for our customers when comparing to established alternatives. There are many FOSS components that address different concerns that we can incorporate and still provide a best-in-class platform.","title":"Decision Outcome"},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/#negative-consequences","text":"High number of components to configure and integrate","title":"Negative Consequences "},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/","text":"Apache Camel as the Default Integration Framework Status: accepted Deciders: @k2merlinsix, @jam01, @mnorton Date: 2020-08 Context and Problem Statement What should be our main/default software framework for integration services? Decision Drivers Modern framework Low learning curve Maturity Solid documentation and community Cost Considered Options Spring Boot Spring Integration Apache Camel Python Flask Alpakka Go Decision Outcome We'll use and extend Apache Camel as our default integration framework. Given the proficiency with Java and Enterprise Integration Patterns within our company and the industry in general, Apache Camel gives our customers a straight forward and low cost migration path from existing implementations. Positive Consequences Existing proficiency with Java and EIP Enables re-use of Maven based artifacts, e.g.: custom exceptions, domain classes, logging layouts, CI/CD pipelines Lower barrier for our customers finding professional resources Links Enterprise Integration Patterns Apache Camel Spring Boot Spring Integration Alpakka","title":"Apache Camel as the Default Integration Framework"},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#apache-camel-as-the-default-integration-framework","text":"Status: accepted Deciders: @k2merlinsix, @jam01, @mnorton Date: 2020-08","title":"Apache Camel as the Default Integration Framework"},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#context-and-problem-statement","text":"What should be our main/default software framework for integration services?","title":"Context and Problem Statement"},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#decision-drivers","text":"Modern framework Low learning curve Maturity Solid documentation and community Cost","title":"Decision Drivers "},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#considered-options","text":"Spring Boot Spring Integration Apache Camel Python Flask Alpakka Go","title":"Considered Options"},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#decision-outcome","text":"We'll use and extend Apache Camel as our default integration framework. Given the proficiency with Java and Enterprise Integration Patterns within our company and the industry in general, Apache Camel gives our customers a straight forward and low cost migration path from existing implementations.","title":"Decision Outcome"},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#positive-consequences","text":"Existing proficiency with Java and EIP Enables re-use of Maven based artifacts, e.g.: custom exceptions, domain classes, logging layouts, CI/CD pipelines Lower barrier for our customers finding professional resources","title":"Positive Consequences "},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#links","text":"Enterprise Integration Patterns Apache Camel Spring Boot Spring Integration Alpakka","title":"Links "},{"location":"adr/0002-spring-boot-as-the-base-application-framework/","text":"Spring Boot as the Base Application Framework Status: accepted Deciders: @k2merlinsix, @jam01, @mnorton Date: 2020-08 Context and Problem Statement Should we add a base application framework to Apache Camel such as Spring Boot or Quarkus? Decision Drivers Speed up development Maturity Solid documentation and community Considered Options Spring Boot Quarkus Decision Outcome We'll use Spring Boot as the base application framework to Apache Camel. We found that Spring Boot's auto-configuration features greatly speed up development time and it is well known and documented Positive Consequences Existing proficiency with Spring Boot as part of our internal bootcamp Access to a bigger ecosystem, e.g.: Spring Cloud Config Links Spring Boot Quarkus","title":"Spring Boot as the Base Application Framework"},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#spring-boot-as-the-base-application-framework","text":"Status: accepted Deciders: @k2merlinsix, @jam01, @mnorton Date: 2020-08","title":"Spring Boot as the Base Application Framework"},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#context-and-problem-statement","text":"Should we add a base application framework to Apache Camel such as Spring Boot or Quarkus?","title":"Context and Problem Statement"},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#decision-drivers","text":"Speed up development Maturity Solid documentation and community","title":"Decision Drivers "},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#considered-options","text":"Spring Boot Quarkus","title":"Considered Options"},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#decision-outcome","text":"We'll use Spring Boot as the base application framework to Apache Camel. We found that Spring Boot's auto-configuration features greatly speed up development time and it is well known and documented","title":"Decision Outcome"},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#positive-consequences","text":"Existing proficiency with Spring Boot as part of our internal bootcamp Access to a bigger ecosystem, e.g.: Spring Cloud Config","title":"Positive Consequences "},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#links","text":"Spring Boot Quarkus","title":"Links "},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/","text":"DataSonnet as the Default Data Transformation Language Status: accepted Deciders: @k2merlinsix, @jam01, @JakeMHughes Date: 2020-08 Context and Problem Statement Does DataSonnet, or can we make it, meet functional and performance requirements for modern integration workloads? Decision Drivers Provide the necessary transformation functions Performance Decision Outcome We'll extend DataSonnet and use it as our default data transformation language. Extending ModusBox' work on top of DataBrick's sjsonnet project we delivered a feature-full and very performant language. Positive Consequences Joint ownership with ModusBox offers great visibility to our company and offerings Apache 2.0 License opens possibilities of adoption by the larger community or existing projects in the same space Negative Consequences Lead time to delivery of the next major release Responsibility of maintaining and stewarding the project with the community Links DataSonnet Mapper Project DataSonnet Jsonnet sjsonnet Project","title":"DataSonnet as the Default Data Transformation Language"},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#datasonnet-as-the-default-data-transformation-language","text":"Status: accepted Deciders: @k2merlinsix, @jam01, @JakeMHughes Date: 2020-08","title":"DataSonnet as the Default Data Transformation Language"},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#context-and-problem-statement","text":"Does DataSonnet, or can we make it, meet functional and performance requirements for modern integration workloads?","title":"Context and Problem Statement"},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#decision-drivers","text":"Provide the necessary transformation functions Performance","title":"Decision Drivers "},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#decision-outcome","text":"We'll extend DataSonnet and use it as our default data transformation language. Extending ModusBox' work on top of DataBrick's sjsonnet project we delivered a feature-full and very performant language.","title":"Decision Outcome"},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#positive-consequences","text":"Joint ownership with ModusBox offers great visibility to our company and offerings Apache 2.0 License opens possibilities of adoption by the larger community or existing projects in the same space","title":"Positive Consequences "},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#negative-consequences","text":"Lead time to delivery of the next major release Responsibility of maintaining and stewarding the project with the community","title":"Negative Consequences "},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#links","text":"DataSonnet Mapper Project DataSonnet Jsonnet sjsonnet Project","title":"Links "},{"location":"adr/0004-opentracing-for-in-process-tracing-api/","text":"OpenTracing for In-Process Tracing API Status: accepted Deciders: @jam01 Date: 2020-08 Context and Problem Statement Which in-process tracing API should we use, OpenTracing or the new project OpenTelemetry? Decision Drivers Existing library integrations Considered Options OpenTracing OpenTelemetry Decision Outcome We'll use OpenTracing for in-process tracing API. Having contributed to the OpenTracing project, and refactoring the camel-opentracing component significantly, OpenTracing provides the most functionality now while OpenTelemetry's design stabilizes. Negative Consequences Will need to migrate to OpenTelemetry as it eventually deprecates OpenTracing Links OpenTracing OpenTelemetry","title":"OpenTracing for In-Process Tracing API"},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#opentracing-for-in-process-tracing-api","text":"Status: accepted Deciders: @jam01 Date: 2020-08","title":"OpenTracing for In-Process Tracing API"},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#context-and-problem-statement","text":"Which in-process tracing API should we use, OpenTracing or the new project OpenTelemetry?","title":"Context and Problem Statement"},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#decision-drivers","text":"Existing library integrations","title":"Decision Drivers "},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#considered-options","text":"OpenTracing OpenTelemetry","title":"Considered Options"},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#decision-outcome","text":"We'll use OpenTracing for in-process tracing API. Having contributed to the OpenTracing project, and refactoring the camel-opentracing component significantly, OpenTracing provides the most functionality now while OpenTelemetry's design stabilizes.","title":"Decision Outcome"},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#negative-consequences","text":"Will need to migrate to OpenTelemetry as it eventually deprecates OpenTracing","title":"Negative Consequences "},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#links","text":"OpenTracing OpenTelemetry","title":"Links "},{"location":"adr/0005-kubernetes-as-the-computing-platform/","text":"Kubernetes as the Computing Platform Status: accepted Deciders: @k2merlinsix, @jam01, Mohammad Naeem Date: 2020-08 Context and Problem Statement What should be our base platform? Decision Drivers Existing tooling and integrations to speed up delivery Decision Outcome We'll use Kubernetes as Tavros's computing platform. The adoption and support for Kubernetes has been evident for some time, most importantly tools like Kops, Helm, Operators, and Ansible's Kubernetes modules, make a great base for us to focus on delivering our platform.","title":"Kubernetes as the Computing Platform"},{"location":"adr/0005-kubernetes-as-the-computing-platform/#kubernetes-as-the-computing-platform","text":"Status: accepted Deciders: @k2merlinsix, @jam01, Mohammad Naeem Date: 2020-08","title":"Kubernetes as the Computing Platform"},{"location":"adr/0005-kubernetes-as-the-computing-platform/#context-and-problem-statement","text":"What should be our base platform?","title":"Context and Problem Statement"},{"location":"adr/0005-kubernetes-as-the-computing-platform/#decision-drivers","text":"Existing tooling and integrations to speed up delivery","title":"Decision Drivers "},{"location":"adr/0005-kubernetes-as-the-computing-platform/#decision-outcome","text":"We'll use Kubernetes as Tavros's computing platform. The adoption and support for Kubernetes has been evident for some time, most importantly tools like Kops, Helm, Operators, and Ansible's Kubernetes modules, make a great base for us to focus on delivering our platform.","title":"Decision Outcome"},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/","text":"Kops to Provision a Kubernetes Cluster Status: accepted Deciders: Mohammad Naeem Date: 2020-08 Context and Problem Statement In order to automate the provisioning of the Kubernetes cluster, what tools should we use? Decision Outcome We'll utilize Kops to provision Kubernetes clusters. Positive Consequences Support for multiple cloud providers Links Kops","title":"Kops to Provision a Kubernetes Cluster"},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/#kops-to-provision-a-kubernetes-cluster","text":"Status: accepted Deciders: Mohammad Naeem Date: 2020-08","title":"Kops to Provision a Kubernetes Cluster"},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/#context-and-problem-statement","text":"In order to automate the provisioning of the Kubernetes cluster, what tools should we use?","title":"Context and Problem Statement"},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/#decision-outcome","text":"We'll utilize Kops to provision Kubernetes clusters.","title":"Decision Outcome"},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/#positive-consequences","text":"Support for multiple cloud providers","title":"Positive Consequences "},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/#links","text":"Kops","title":"Links "},{"location":"adr/0007-flux-to-provide-platform-gitops/","text":"Flux v2 Toolkit to Provide Platform GitOps Status: accepted Deciders: Mohammad Naeem, @jam01, @rmccright-ms3 Date: 2020-09 Context and Problem Statement We want to enable continuous delivery of platform and application workloads in a GitOps way. What tools should we use? Decision Drivers Simplicity Integration with tools like Helm and Kustomize Considered Options Flux Flux v2 GitOps Toolkit Argo CD Decision Outcome We'll use Flux v2 GitOps Toolkit to enable continuous delivery of the platform and application workloads. Having had experience with Flux v1 internally, along with the newer features of v2 while still maintaining a simple workflow, it's the more appropriate tool. Positive Consequences Flux v2 Custom Resource Definitions make it easy to utilize Flux Helm functionality before the source Git repository is up Alert features through integrations like Slack Negative Consequences Have to be careful with the 'chicken and egg problem' between Flux managing the platform, and provisioning platform components through Flux Links GitOps Guide Flux v2 GitOps Toolkit Argo CD","title":"Flux v2 Toolkit to Provide Platform GitOps"},{"location":"adr/0007-flux-to-provide-platform-gitops/#flux-v2-toolkit-to-provide-platform-gitops","text":"Status: accepted Deciders: Mohammad Naeem, @jam01, @rmccright-ms3 Date: 2020-09","title":"Flux v2 Toolkit to Provide Platform GitOps"},{"location":"adr/0007-flux-to-provide-platform-gitops/#context-and-problem-statement","text":"We want to enable continuous delivery of platform and application workloads in a GitOps way. What tools should we use?","title":"Context and Problem Statement"},{"location":"adr/0007-flux-to-provide-platform-gitops/#decision-drivers","text":"Simplicity Integration with tools like Helm and Kustomize","title":"Decision Drivers "},{"location":"adr/0007-flux-to-provide-platform-gitops/#considered-options","text":"Flux Flux v2 GitOps Toolkit Argo CD","title":"Considered Options"},{"location":"adr/0007-flux-to-provide-platform-gitops/#decision-outcome","text":"We'll use Flux v2 GitOps Toolkit to enable continuous delivery of the platform and application workloads. Having had experience with Flux v1 internally, along with the newer features of v2 while still maintaining a simple workflow, it's the more appropriate tool.","title":"Decision Outcome"},{"location":"adr/0007-flux-to-provide-platform-gitops/#positive-consequences","text":"Flux v2 Custom Resource Definitions make it easy to utilize Flux Helm functionality before the source Git repository is up Alert features through integrations like Slack","title":"Positive Consequences "},{"location":"adr/0007-flux-to-provide-platform-gitops/#negative-consequences","text":"Have to be careful with the 'chicken and egg problem' between Flux managing the platform, and provisioning platform components through Flux","title":"Negative Consequences "},{"location":"adr/0007-flux-to-provide-platform-gitops/#links","text":"GitOps Guide Flux v2 GitOps Toolkit Argo CD","title":"Links "},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/","text":"Kubeseal to Securely Manage Secrets in GitOps Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-09 Context and Problem Statement In order to store secrets safely in a public or private Git repository, what tool do we use? Decision Drivers Simplicity Integration with Flux Decision Outcome We'll use Bitnami's Sealed Secrets controller to securely manage secrets in GitOps. The sealed secrets can be decrypted only by the controller running in your cluster and nobody else can obtain the original secret, even if they have access to the Git repository. Links Flux Sealed Secrets recommendation Sealed Secrets project","title":"Kubeseal to Securely Manage Secrets in GitOps"},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/#kubeseal-to-securely-manage-secrets-in-gitops","text":"Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-09","title":"Kubeseal to Securely Manage Secrets in GitOps"},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/#context-and-problem-statement","text":"In order to store secrets safely in a public or private Git repository, what tool do we use?","title":"Context and Problem Statement"},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/#decision-drivers","text":"Simplicity Integration with Flux","title":"Decision Drivers "},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/#decision-outcome","text":"We'll use Bitnami's Sealed Secrets controller to securely manage secrets in GitOps. The sealed secrets can be decrypted only by the controller running in your cluster and nobody else can obtain the original secret, even if they have access to the Git repository.","title":"Decision Outcome"},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/#links","text":"Flux Sealed Secrets recommendation Sealed Secrets project","title":"Links "},{"location":"adr/0009-keycloak-for-indetity-and-access-management/","text":"Keycloak for Identity and Access Management Status: accepted Deciders: Mohammad Naeem Date: 2020-08 Context and Problem Statement To provide user identity and access management, what component do we use? Decision Outcome We'll use Keycloak for identity and access management. Links Keycloak","title":"Keycloak for Identity and Access Management"},{"location":"adr/0009-keycloak-for-indetity-and-access-management/#keycloak-for-identity-and-access-management","text":"Status: accepted Deciders: Mohammad Naeem Date: 2020-08","title":"Keycloak for Identity and Access Management"},{"location":"adr/0009-keycloak-for-indetity-and-access-management/#context-and-problem-statement","text":"To provide user identity and access management, what component do we use?","title":"Context and Problem Statement"},{"location":"adr/0009-keycloak-for-indetity-and-access-management/#decision-outcome","text":"We'll use Keycloak for identity and access management.","title":"Decision Outcome"},{"location":"adr/0009-keycloak-for-indetity-and-access-management/#links","text":"Keycloak","title":"Links "},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/","text":"Kong as Kubernetes Ingress Controller and API Gateway Status: accepted Deciders: @k2merlinsix Date: 2020-08 Context and Problem Statement Which component should serve as our Kubernetes Ingress Controller, to do load balancing and proxying? Which component should serve as our API Gateway? How easy is it to tie into the application networking of the Ingress Controller? Decision Drivers Network performance Feature set of API Gateway Considered Options NGINX Kong Apigee KrakenD Decision Outcome Kong will be our Kubernetes Ingress Controller and API Gateway as well. Having a single component perform both functions should make it easier to maintain and more performant. Positive Consequences Enterprise Edition pricing is more cost effective than some alternatives Negative Consequences Lua as the main language for developing plugins Links Kong NGINX Apigee KrakenD","title":"Kong as Kubernetes Ingress Controller and API Gateway"},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#kong-as-kubernetes-ingress-controller-and-api-gateway","text":"Status: accepted Deciders: @k2merlinsix Date: 2020-08","title":"Kong as Kubernetes Ingress Controller and API Gateway"},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#context-and-problem-statement","text":"Which component should serve as our Kubernetes Ingress Controller, to do load balancing and proxying? Which component should serve as our API Gateway? How easy is it to tie into the application networking of the Ingress Controller?","title":"Context and Problem Statement"},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#decision-drivers","text":"Network performance Feature set of API Gateway","title":"Decision Drivers "},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#considered-options","text":"NGINX Kong Apigee KrakenD","title":"Considered Options"},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#decision-outcome","text":"Kong will be our Kubernetes Ingress Controller and API Gateway as well. Having a single component perform both functions should make it easier to maintain and more performant.","title":"Decision Outcome"},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#positive-consequences","text":"Enterprise Edition pricing is more cost effective than some alternatives","title":"Positive Consequences "},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#negative-consequences","text":"Lua as the main language for developing plugins","title":"Negative Consequences "},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#links","text":"Kong NGINX Apigee KrakenD","title":"Links "},{"location":"adr/0011-postgresql-as-the-platform%27s-default-database/","text":"PostgreSQL as the Platform's Default Database Status: accepted Deciders: Mohammad Naeem Date: 2020-09 Context and Problem Statement Some components require a PostgreSQL database, how do we accommodate each one and future components? Decision Outcome We'll use PostgreSQL as the platform's default database. Since Gitea and Keycloak require a PostgreSQL database we'll setup a single server and dynamically create what's needed for each component that uses PostgreSQL. If it's an option to use PostgreSQL for future components, we'll use that. Positive Consequences Single instance to backup, and restore in case of a disaster recovery","title":"PostgreSQL as the Platform's Default Database"},{"location":"adr/0011-postgresql-as-the-platform%27s-default-database/#postgresql-as-the-platforms-default-database","text":"Status: accepted Deciders: Mohammad Naeem Date: 2020-09","title":"PostgreSQL as the Platform's Default Database"},{"location":"adr/0011-postgresql-as-the-platform%27s-default-database/#context-and-problem-statement","text":"Some components require a PostgreSQL database, how do we accommodate each one and future components?","title":"Context and Problem Statement"},{"location":"adr/0011-postgresql-as-the-platform%27s-default-database/#decision-outcome","text":"We'll use PostgreSQL as the platform's default database. Since Gitea and Keycloak require a PostgreSQL database we'll setup a single server and dynamically create what's needed for each component that uses PostgreSQL. If it's an option to use PostgreSQL for future components, we'll use that.","title":"Decision Outcome"},{"location":"adr/0011-postgresql-as-the-platform%27s-default-database/#positive-consequences","text":"Single instance to backup, and restore in case of a disaster recovery","title":"Positive Consequences "},{"location":"adr/0012-gitea-for-a-lightweight-git-server/","text":"Gitea for a Lightweight Git Server Status: accepted Deciders: Mohammad Naeem, @jam01 Date: 2020-09 Context and Problem Statement Given that Flux requires a Git repository to provide GitOps, and that our CI/CD pipelines will require the same for continuous delivery, what component should be our default/bundled Git server? Decision Drivers Lightweight as we expect a number of customers will already have a Git saas Easy to integrate Considered Options GitLab Gitea Decision Outcome We'll use Gitea as our bundled and lightweight Git server. Gitea is more lightweight than the alternatives, and has the necessary APIs for us to integrate with. Negative Consequences No out of the box integration with Flux Links Gitea GitLab","title":"Gitea for a Lightweight Git Server"},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#gitea-for-a-lightweight-git-server","text":"Status: accepted Deciders: Mohammad Naeem, @jam01 Date: 2020-09","title":"Gitea for a Lightweight Git Server"},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#context-and-problem-statement","text":"Given that Flux requires a Git repository to provide GitOps, and that our CI/CD pipelines will require the same for continuous delivery, what component should be our default/bundled Git server?","title":"Context and Problem Statement"},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#decision-drivers","text":"Lightweight as we expect a number of customers will already have a Git saas Easy to integrate","title":"Decision Drivers "},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#considered-options","text":"GitLab Gitea","title":"Considered Options"},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#decision-outcome","text":"We'll use Gitea as our bundled and lightweight Git server. Gitea is more lightweight than the alternatives, and has the necessary APIs for us to integrate with.","title":"Decision Outcome"},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#negative-consequences","text":"No out of the box integration with Flux","title":"Negative Consequences "},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#links","text":"Gitea GitLab","title":"Links "},{"location":"adr/0013-kuma-for-service-mesh/","text":"Kuma for Service Mesh Status: accepted Deciders: @k2merlinsix Date: 2020-08 Context and Problem Statement Service meshes helps to address cross cutting concerns so that application developers don't have to, what service mesh component should we use? Decision Drivers Performance Feature set Considered Options Kuma Istio Decision Outcome We'll use Kuma for service mesh. Positive Consequences Seamless integration with Kong Links Kuma","title":"Kuma for Service Mesh"},{"location":"adr/0013-kuma-for-service-mesh/#kuma-for-service-mesh","text":"Status: accepted Deciders: @k2merlinsix Date: 2020-08","title":"Kuma for Service Mesh"},{"location":"adr/0013-kuma-for-service-mesh/#context-and-problem-statement","text":"Service meshes helps to address cross cutting concerns so that application developers don't have to, what service mesh component should we use?","title":"Context and Problem Statement"},{"location":"adr/0013-kuma-for-service-mesh/#decision-drivers","text":"Performance Feature set","title":"Decision Drivers "},{"location":"adr/0013-kuma-for-service-mesh/#considered-options","text":"Kuma Istio","title":"Considered Options"},{"location":"adr/0013-kuma-for-service-mesh/#decision-outcome","text":"We'll use Kuma for service mesh.","title":"Decision Outcome"},{"location":"adr/0013-kuma-for-service-mesh/#positive-consequences","text":"Seamless integration with Kong","title":"Positive Consequences "},{"location":"adr/0013-kuma-for-service-mesh/#links","text":"Kuma","title":"Links "},{"location":"adr/0014-jenkins-for-continuous-integration/","text":"Jenkins for Continuous Integration Status: accepted Deciders: @jam01 Date: 2020-09 Context and Problem Statement We'll be using Flux for continuous deployment, leaving us to choose a component for continuous delivery, i.e.: building and running tests against artifacts before deploying. Decision Drivers Existing proficiency and pipelines Simplicity Considered Options Jenkins Jenkins X Tekton Decision Outcome We'll use Jenkins as our CI component. Since we're using Flux for deployment we don't need too Kubernetes specific features like Jenkins X has, and Jenkins can support for complex pipelines than Tekton. Positive Consequences Can re-use existing pipelines with little modification Negative Consequences Less 'modern' than other options Links Jenkins X Jenkins Tekton Project","title":"Jenkins for Continuous Integration"},{"location":"adr/0014-jenkins-for-continuous-integration/#jenkins-for-continuous-integration","text":"Status: accepted Deciders: @jam01 Date: 2020-09","title":"Jenkins for Continuous Integration"},{"location":"adr/0014-jenkins-for-continuous-integration/#context-and-problem-statement","text":"We'll be using Flux for continuous deployment, leaving us to choose a component for continuous delivery, i.e.: building and running tests against artifacts before deploying.","title":"Context and Problem Statement"},{"location":"adr/0014-jenkins-for-continuous-integration/#decision-drivers","text":"Existing proficiency and pipelines Simplicity","title":"Decision Drivers "},{"location":"adr/0014-jenkins-for-continuous-integration/#considered-options","text":"Jenkins Jenkins X Tekton","title":"Considered Options"},{"location":"adr/0014-jenkins-for-continuous-integration/#decision-outcome","text":"We'll use Jenkins as our CI component. Since we're using Flux for deployment we don't need too Kubernetes specific features like Jenkins X has, and Jenkins can support for complex pipelines than Tekton.","title":"Decision Outcome"},{"location":"adr/0014-jenkins-for-continuous-integration/#positive-consequences","text":"Can re-use existing pipelines with little modification","title":"Positive Consequences "},{"location":"adr/0014-jenkins-for-continuous-integration/#negative-consequences","text":"Less 'modern' than other options","title":"Negative Consequences "},{"location":"adr/0014-jenkins-for-continuous-integration/#links","text":"Jenkins X Jenkins Tekton Project","title":"Links "},{"location":"adr/0015-sonarqube-for-application-static-code-analysis/","text":"Sonarqube for Application Static Code Analysis Status: accepted Deciders: @jam01 Date: 2020-10 Context and Problem Statement We understand the value of automated code qualitative analysis and that it's often an afterthought. Should we bundle a static code analysis component? Decision Outcome We'll use Sonarqube for application static code analysis. As part of our effort to offer a ready best-practices platform we'll setup Sonarqube and tie it into template or default CI/CD pipelines. Links Sonarqube","title":"Sonarqube for Application Static Code Analysis"},{"location":"adr/0015-sonarqube-for-application-static-code-analysis/#sonarqube-for-application-static-code-analysis","text":"Status: accepted Deciders: @jam01 Date: 2020-10","title":"Sonarqube for Application Static Code Analysis"},{"location":"adr/0015-sonarqube-for-application-static-code-analysis/#context-and-problem-statement","text":"We understand the value of automated code qualitative analysis and that it's often an afterthought. Should we bundle a static code analysis component?","title":"Context and Problem Statement"},{"location":"adr/0015-sonarqube-for-application-static-code-analysis/#decision-outcome","text":"We'll use Sonarqube for application static code analysis. As part of our effort to offer a ready best-practices platform we'll setup Sonarqube and tie it into template or default CI/CD pipelines.","title":"Decision Outcome"},{"location":"adr/0015-sonarqube-for-application-static-code-analysis/#links","text":"Sonarqube","title":"Links "},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/","text":"Elastic Cloud for Observability Data Aggregation and Visualization Status: accepted Deciders: @jam01, @k2merlinsix Date: 2020-10 Context and Problem Statement In a microservices architectural system observability is imperative, what components should we use for that? Decision Drivers Cost Feature set Considered Options Elastic Cloud Jaeger Decision Outcome We'll use Elastic Cloud on Kubernetes for Observability Data Aggregation and Visualization. Elastic, Logstash and Kibana offer what we need for logging data. Aditionally, we can use APM server to get trace data from either Elastic's own apm agent or a Jaeger client. Positive Consequences A single pane of glass for observability Less components to coordinate Negative Consequences Valuable service dependency DAG only on license subscription Links Elastic Cloud on Kubernetes Elastic Stack Pricing Elastic with Jaeger ECK project Follow up decision to use Jaeger for Tracing ADR-0017","title":"Elastic Cloud for Observability Data Aggregation and Visualization"},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#elastic-cloud-for-observability-data-aggregation-and-visualization","text":"Status: accepted Deciders: @jam01, @k2merlinsix Date: 2020-10","title":"Elastic Cloud for Observability Data Aggregation and Visualization"},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#context-and-problem-statement","text":"In a microservices architectural system observability is imperative, what components should we use for that?","title":"Context and Problem Statement"},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#decision-drivers","text":"Cost Feature set","title":"Decision Drivers "},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#considered-options","text":"Elastic Cloud Jaeger","title":"Considered Options"},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#decision-outcome","text":"We'll use Elastic Cloud on Kubernetes for Observability Data Aggregation and Visualization. Elastic, Logstash and Kibana offer what we need for logging data. Aditionally, we can use APM server to get trace data from either Elastic's own apm agent or a Jaeger client.","title":"Decision Outcome"},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#positive-consequences","text":"A single pane of glass for observability Less components to coordinate","title":"Positive Consequences "},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#negative-consequences","text":"Valuable service dependency DAG only on license subscription","title":"Negative Consequences "},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#links","text":"Elastic Cloud on Kubernetes Elastic Stack Pricing Elastic with Jaeger ECK project Follow up decision to use Jaeger for Tracing ADR-0017","title":"Links "},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/","text":"Jaeger for Tracing with Elasticsearch Backend Status: accepted Deciders: @jam01 Date: 2020-10 Context and Problem Statement In a microservices architectural system observability is imperative, and Elastic Stack Open Source does not offer service dependency DAG. Can/should we fill in that feature with another component? Decision Drivers Cost Feature set Considered Options Elastic Stack Jaeger Decision Outcome We'll use Jaeger by default for tracing data, with Elasticsearch as the backend. Using Jaeger enables a service DAG that's very valuable and using elasticsearch as the backend enables data aggregation and analysis in Kibana. Positive Consequences Value of the service DAG dependency graph Maintain the ability to cross reference logs and trace data in a single pane Jaeger has closer participation to OpenTracing/Telemetry projects than Elastic Negative Consequences Another integration point Must correlate logs and traces in Camel through MDC Links Jaeger Elastic with Jaeger Jaeger Operator Article Jaeger Elasticsearch and Kibana Article Distributed Tracing with Jaeger and the ELK Stack Article Exploring Jaeger traces with Elastic APM","title":"Jaeger for Tracing with Elasticsearch Backend"},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#jaeger-for-tracing-with-elasticsearch-backend","text":"Status: accepted Deciders: @jam01 Date: 2020-10","title":"Jaeger for Tracing with Elasticsearch Backend"},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#context-and-problem-statement","text":"In a microservices architectural system observability is imperative, and Elastic Stack Open Source does not offer service dependency DAG. Can/should we fill in that feature with another component?","title":"Context and Problem Statement"},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#decision-drivers","text":"Cost Feature set","title":"Decision Drivers "},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#considered-options","text":"Elastic Stack Jaeger","title":"Considered Options"},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#decision-outcome","text":"We'll use Jaeger by default for tracing data, with Elasticsearch as the backend. Using Jaeger enables a service DAG that's very valuable and using elasticsearch as the backend enables data aggregation and analysis in Kibana.","title":"Decision Outcome"},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#positive-consequences","text":"Value of the service DAG dependency graph Maintain the ability to cross reference logs and trace data in a single pane Jaeger has closer participation to OpenTracing/Telemetry projects than Elastic","title":"Positive Consequences "},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#negative-consequences","text":"Another integration point Must correlate logs and traces in Camel through MDC","title":"Negative Consequences "},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#links","text":"Jaeger Elastic with Jaeger Jaeger Operator Article Jaeger Elasticsearch and Kibana Article Distributed Tracing with Jaeger and the ELK Stack Article Exploring Jaeger traces with Elastic APM","title":"Links "},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/","text":"Nexus Repository Manager for Artifact Management Status: accepted Deciders: @jam01, @k2merlinsix Date: 2020-10 Context and Problem Statement In a solid continuous delivery configuration an enterprise needs to have a history of application builds an ability to rollback deployments as necessary. What artifact manager component should we use? Decision Drivers Stability Considered Options Nexus Repository Manager JFrog Artifactory Decision Outcome We'll use Nexus repository manager for artifact management. Sonatype is one of the main stewards/contributors of Maven and therefore their platform is one of the most mature. In order to deliver a best practices continuous delivery configuration we'll tie in Nexus into our CI/CD pipelines. Positive Consequences Available commercial support from Nexus Links Nexus Repository Manager OSS JFrog Artifactory","title":"Nexus Repository Manager for Artifact Management"},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#nexus-repository-manager-for-artifact-management","text":"Status: accepted Deciders: @jam01, @k2merlinsix Date: 2020-10","title":"Nexus Repository Manager for Artifact Management"},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#context-and-problem-statement","text":"In a solid continuous delivery configuration an enterprise needs to have a history of application builds an ability to rollback deployments as necessary. What artifact manager component should we use?","title":"Context and Problem Statement"},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#decision-drivers","text":"Stability","title":"Decision Drivers "},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#considered-options","text":"Nexus Repository Manager JFrog Artifactory","title":"Considered Options"},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#decision-outcome","text":"We'll use Nexus repository manager for artifact management. Sonatype is one of the main stewards/contributors of Maven and therefore their platform is one of the most mature. In order to deliver a best practices continuous delivery configuration we'll tie in Nexus into our CI/CD pipelines.","title":"Decision Outcome"},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#positive-consequences","text":"Available commercial support from Nexus","title":"Positive Consequences "},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#links","text":"Nexus Repository Manager OSS JFrog Artifactory","title":"Links "},{"location":"adr/0019-prefer-daemonsets-over-sidecars/","text":"Prefer Daemonsets over Sidecars Status: accepted Deciders: @jam01 Date: 2020-11 Context and Problem Statement Kubernetes supports deploying workloads as daemonsets or sidecars. For example, when deploying Jaeger Agent for reporting trace data, in a DaemonSet strategy there will be a single Agent deployment in every Kubernetes node, so all applications' Jaeger Client in a single node share the same agent. Conversely, in a side-car strategy each application will have its own dedicated Jaeger Agent deployment, requiring extra resources. Which should we favor for cross cutting concern deployments? Decision Drivers Cost Considered Options Sidecars Daemonsets Decision Outcome We'll prefer daemonsets over sidecar deployments whenever there is the option. Given that Tavros is designed as a single tenant platform there are computing resource savings when using daemonsets (deployment per node) vs sidecars (deployment per service). Positive Consequences Less resource utilization Negative Consequences Would have to refactor in order to support multi-tenancy Links DaemonSet See about single tenancy in ADR-0029","title":"Prefer Daemonsets over Sidecars"},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#prefer-daemonsets-over-sidecars","text":"Status: accepted Deciders: @jam01 Date: 2020-11","title":"Prefer Daemonsets over Sidecars"},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#context-and-problem-statement","text":"Kubernetes supports deploying workloads as daemonsets or sidecars. For example, when deploying Jaeger Agent for reporting trace data, in a DaemonSet strategy there will be a single Agent deployment in every Kubernetes node, so all applications' Jaeger Client in a single node share the same agent. Conversely, in a side-car strategy each application will have its own dedicated Jaeger Agent deployment, requiring extra resources. Which should we favor for cross cutting concern deployments?","title":"Context and Problem Statement"},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#decision-drivers","text":"Cost","title":"Decision Drivers "},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#considered-options","text":"Sidecars Daemonsets","title":"Considered Options"},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#decision-outcome","text":"We'll prefer daemonsets over sidecar deployments whenever there is the option. Given that Tavros is designed as a single tenant platform there are computing resource savings when using daemonsets (deployment per node) vs sidecars (deployment per service).","title":"Decision Outcome"},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#positive-consequences","text":"Less resource utilization","title":"Positive Consequences "},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#negative-consequences","text":"Would have to refactor in order to support multi-tenancy","title":"Negative Consequences "},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#links","text":"DaemonSet See about single tenancy in ADR-0029","title":"Links "},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/","text":"Spring Cloud Config for Application Configuration Management Status: accepted Deciders: @jam01 Date: 2020-11 Context and Problem Statement In a solid continuous delivery configuration an enterprise needs to manage application configuration properties independently of the application source code. What component should we use? Decision Drivers Flexible backend support Easy integration with Camel and Spring Boot Considered Options Spring Cloud Config Decision Outcome We'll use spring cloud config as our application configuration manager. Spring cloud config is very simple to integrate since we chose Spring Boot as our base application framework. We'll use a Git backend from Gitea which will allow our customers to keep a history of changes and enable rollback. Positive Consequences Simple integration to Spring Boot Links Spring Cloud Config","title":"Spring Cloud Config for Application Configuration Management"},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#spring-cloud-config-for-application-configuration-management","text":"Status: accepted Deciders: @jam01 Date: 2020-11","title":"Spring Cloud Config for Application Configuration Management"},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#context-and-problem-statement","text":"In a solid continuous delivery configuration an enterprise needs to manage application configuration properties independently of the application source code. What component should we use?","title":"Context and Problem Statement"},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#decision-drivers","text":"Flexible backend support Easy integration with Camel and Spring Boot","title":"Decision Drivers "},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#considered-options","text":"Spring Cloud Config","title":"Considered Options"},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#decision-outcome","text":"We'll use spring cloud config as our application configuration manager. Spring cloud config is very simple to integrate since we chose Spring Boot as our base application framework. We'll use a Git backend from Gitea which will allow our customers to keep a history of changes and enable rollback.","title":"Decision Outcome"},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#positive-consequences","text":"Simple integration to Spring Boot","title":"Positive Consequences "},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#links","text":"Spring Cloud Config","title":"Links "},{"location":"adr/0021-prefer-kong-enterprise-edition/","text":"Prefer Kong Enterprise Edition Status: accepted Deciders: @k2merlinsix Date: 2020-11 Context and Problem Statement Given that we've chosen Kong as our Ingress Controller and API Gateway, but we're still missing an API Manager and Developer Portal. Decision Drivers Feature set Decision Outcome We'll recommend Kong enterprise edition in order to enable API manager and developer portal features. API manager and developer portal features are not necessary but they are valuable, supporting Kong EE as an optional component is an acceptable compromise to offer that value to our customers. Positive Consequences Single component for all API related functionality Negative Consequences Requires a license Links Kong Enterprise","title":"Prefer Kong Enterprise Edition"},{"location":"adr/0021-prefer-kong-enterprise-edition/#prefer-kong-enterprise-edition","text":"Status: accepted Deciders: @k2merlinsix Date: 2020-11","title":"Prefer Kong Enterprise Edition"},{"location":"adr/0021-prefer-kong-enterprise-edition/#context-and-problem-statement","text":"Given that we've chosen Kong as our Ingress Controller and API Gateway, but we're still missing an API Manager and Developer Portal.","title":"Context and Problem Statement"},{"location":"adr/0021-prefer-kong-enterprise-edition/#decision-drivers","text":"Feature set","title":"Decision Drivers "},{"location":"adr/0021-prefer-kong-enterprise-edition/#decision-outcome","text":"We'll recommend Kong enterprise edition in order to enable API manager and developer portal features. API manager and developer portal features are not necessary but they are valuable, supporting Kong EE as an optional component is an acceptable compromise to offer that value to our customers.","title":"Decision Outcome"},{"location":"adr/0021-prefer-kong-enterprise-edition/#positive-consequences","text":"Single component for all API related functionality","title":"Positive Consequences "},{"location":"adr/0021-prefer-kong-enterprise-edition/#negative-consequences","text":"Requires a license","title":"Negative Consequences "},{"location":"adr/0021-prefer-kong-enterprise-edition/#links","text":"Kong Enterprise","title":"Links "},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/","text":"Use Ansible as Provisioning Engine Status: accepted Deciders: Mohammad Naeem, @jam01 Date: 2020-11 Context and Problem Statement [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] Decision Drivers Flexibility Maintainability Considered Options Bash Ansible Decision Outcome We'll use Ansible as the provisioning engine. Ansible modules, variable handling, Jinja 2 templating, and Kubernetes module make it easier to install and configure components. Helm Charts use the same templating language, and the Operator SDK has support for Ansible, if we decide to build an Operator. Positive Consequences Ansible being a desired-state engine enables idempotency Simpler setup Negative Consequences A learning curve to Ansible, playbooks, roles, etc Links Ansible Kubernetes Ansible Ansible Operator SDK","title":"Use Ansible as Provisioning Engine"},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#use-ansible-as-provisioning-engine","text":"Status: accepted Deciders: Mohammad Naeem, @jam01 Date: 2020-11","title":"Use Ansible as Provisioning Engine"},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#context-and-problem-statement","text":"[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]","title":"Context and Problem Statement"},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#decision-drivers","text":"Flexibility Maintainability","title":"Decision Drivers "},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#considered-options","text":"Bash Ansible","title":"Considered Options"},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#decision-outcome","text":"We'll use Ansible as the provisioning engine. Ansible modules, variable handling, Jinja 2 templating, and Kubernetes module make it easier to install and configure components. Helm Charts use the same templating language, and the Operator SDK has support for Ansible, if we decide to build an Operator.","title":"Decision Outcome"},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#positive-consequences","text":"Ansible being a desired-state engine enables idempotency Simpler setup","title":"Positive Consequences "},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#negative-consequences","text":"A learning curve to Ansible, playbooks, roles, etc","title":"Negative Consequences "},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#links","text":"Ansible Kubernetes Ansible Ansible Operator SDK","title":"Links "},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/","text":"Helm and Operators for Component Installation and Management Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11 Context and Problem Statement We're currently installing components ad-hoc through directly modified Kubernetes manifests from helm charts. This removes the ability to use Helm's upgrade features. This is done because we can't use Helm until Flux is up and running, and Flux depends on other components being installed before. Decision Drivers Ability to use Helm upgrades Minimize hand crafted manifests Considered Options Helm releases through Flux v2 Operators Decision Outcome We'll use Helm releases and Operators custom resources for component installation and management. Given that we're now using Flux v2 we can use their Helm release functionality before any Git repository is available, this way we can do Helm releases through Flux and eventually commit them to Git to enable GitOps from that point on. Kubernets Operators enable a higher level of component lifecycle management, these should always be preferred to Helm whenever available. Positive Consequences Upon provisioning the platform it will be entirely driven by GitOps Will be able to do component upgrades through Helm releases Less custom code by using official functionality as available Links Helm Operator Pattern Operator Hub Flux v2 Helm Controller","title":"Helm and Operators for Component Installation and Management"},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#helm-and-operators-for-component-installation-and-management","text":"Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11","title":"Helm and Operators for Component Installation and Management"},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#context-and-problem-statement","text":"We're currently installing components ad-hoc through directly modified Kubernetes manifests from helm charts. This removes the ability to use Helm's upgrade features. This is done because we can't use Helm until Flux is up and running, and Flux depends on other components being installed before.","title":"Context and Problem Statement"},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#decision-drivers","text":"Ability to use Helm upgrades Minimize hand crafted manifests","title":"Decision Drivers "},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#considered-options","text":"Helm releases through Flux v2 Operators","title":"Considered Options"},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#decision-outcome","text":"We'll use Helm releases and Operators custom resources for component installation and management. Given that we're now using Flux v2 we can use their Helm release functionality before any Git repository is available, this way we can do Helm releases through Flux and eventually commit them to Git to enable GitOps from that point on. Kubernets Operators enable a higher level of component lifecycle management, these should always be preferred to Helm whenever available.","title":"Decision Outcome"},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#positive-consequences","text":"Upon provisioning the platform it will be entirely driven by GitOps Will be able to do component upgrades through Helm releases Less custom code by using official functionality as available","title":"Positive Consequences "},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#links","text":"Helm Operator Pattern Operator Hub Flux v2 Helm Controller","title":"Links "},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/","text":"Use Ansible Collection to Structure and Package Ansible Code Status: accepted Deciders: @jam01 Date: 2020-11 Context and Problem Statement How do we package multiple playbooks, plugins, and how do we distribute them when creating a customer's platform? Designing pre-configured components and their extension points through Kustomizations is increasingly complex. Is there a way to simplify and make it more maintainable? Decision Drivers Maintainability of the code Easier to add customer configurability of components Support for distribution Considered Options Ansible Collection Decision Outcome We'll use Ansible Collection structure and packaging for our Ansible code. We'll refactor the platform from multiple layers of Kustomizations to be entirely driven by an Ansible Collection. Ansible is migrating all their re-usable artifacts to be Collections so that seems to be where all support and tooling is to be found, including distribution from a git repository. Moving the component configuration and extension points to Ansible makes it much easier to structure and maintain, as well as add points of extension. Positive Consequences Higher maintainability as it's all Ansible Easier to add customer configurability as everything can be templated Negative Consequences A bit more complexity Links Using Collections Blog The Future of Ansible Content Delivery Blog Hands on with Ansible collections","title":"Use Ansible Collection to Structure and Package Ansible Code"},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#use-ansible-collection-to-structure-and-package-ansible-code","text":"Status: accepted Deciders: @jam01 Date: 2020-11","title":"Use Ansible Collection to Structure and Package Ansible Code"},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#context-and-problem-statement","text":"How do we package multiple playbooks, plugins, and how do we distribute them when creating a customer's platform? Designing pre-configured components and their extension points through Kustomizations is increasingly complex. Is there a way to simplify and make it more maintainable?","title":"Context and Problem Statement"},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#decision-drivers","text":"Maintainability of the code Easier to add customer configurability of components Support for distribution","title":"Decision Drivers "},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#considered-options","text":"Ansible Collection","title":"Considered Options"},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#decision-outcome","text":"We'll use Ansible Collection structure and packaging for our Ansible code. We'll refactor the platform from multiple layers of Kustomizations to be entirely driven by an Ansible Collection. Ansible is migrating all their re-usable artifacts to be Collections so that seems to be where all support and tooling is to be found, including distribution from a git repository. Moving the component configuration and extension points to Ansible makes it much easier to structure and maintain, as well as add points of extension.","title":"Decision Outcome"},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#positive-consequences","text":"Higher maintainability as it's all Ansible Easier to add customer configurability as everything can be templated","title":"Positive Consequences "},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#negative-consequences","text":"A bit more complexity","title":"Negative Consequences "},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#links","text":"Using Collections Blog The Future of Ansible Content Delivery Blog Hands on with Ansible collections","title":"Links "},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/","text":"Setup Sandbox and Production Kuma Meshes and Kong Ingress Controllers Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11 Context and Problem Statement What should be our default service mesh and application environments look like? Given that we'll enable mutual TLS on the meshes, each mesh will require a Kong Ingress Controller which in turn requires a Load Balancer from the cloud provider. Decision Drivers Must reflect best practice configuration Segregate production from non-production workloads as much as possible Satisfy 'common' enterprise requirements Cost Considered Options One dev, one test, and one production environment, all segregated through Kuma meshes Dev and test environments separated by kuma meshes in one cluster, production in another Dev and test environments in one 'sandbox' kuma mesh, production in a 'production' mesh Decision Outcome We'll setup a Sandbox and Production Kuma mesh, sandbox will have a dev and test namespaced environment, and production will have the production namespaced environment. The service mesh and mutual TLS plugin enforces the segregation of non-production (aka sandbox) environments and the production environment to where we don't fuctionally need a separate cluster. Moreover aggregating dev and test into a sandbox mesh saves us provisioning a load balancer for each environment, so we could support any arbitrary environment configuration needs. Positive Consequences Reduced cost of extra load balancer Reduced complexity of dedicated cluster Negative Consequences Learning curve of service mesh mutual TLS enforcement Sandboxed environments can still communicate with each other Links Kuma mTLS Kuma Gateway","title":"Setup Sandbox and Production Kuma Meshes and Kong Ingress Controllers"},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers","text":"Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11","title":"Setup Sandbox and Production Kuma Meshes and Kong Ingress Controllers"},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#context-and-problem-statement","text":"What should be our default service mesh and application environments look like? Given that we'll enable mutual TLS on the meshes, each mesh will require a Kong Ingress Controller which in turn requires a Load Balancer from the cloud provider.","title":"Context and Problem Statement"},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#decision-drivers","text":"Must reflect best practice configuration Segregate production from non-production workloads as much as possible Satisfy 'common' enterprise requirements Cost","title":"Decision Drivers "},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#considered-options","text":"One dev, one test, and one production environment, all segregated through Kuma meshes Dev and test environments separated by kuma meshes in one cluster, production in another Dev and test environments in one 'sandbox' kuma mesh, production in a 'production' mesh","title":"Considered Options"},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#decision-outcome","text":"We'll setup a Sandbox and Production Kuma mesh, sandbox will have a dev and test namespaced environment, and production will have the production namespaced environment. The service mesh and mutual TLS plugin enforces the segregation of non-production (aka sandbox) environments and the production environment to where we don't fuctionally need a separate cluster. Moreover aggregating dev and test into a sandbox mesh saves us provisioning a load balancer for each environment, so we could support any arbitrary environment configuration needs.","title":"Decision Outcome"},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#positive-consequences","text":"Reduced cost of extra load balancer Reduced complexity of dedicated cluster","title":"Positive Consequences "},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#negative-consequences","text":"Learning curve of service mesh mutual TLS enforcement Sandboxed environments can still communicate with each other","title":"Negative Consequences "},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#links","text":"Kuma mTLS Kuma Gateway","title":"Links "},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/","text":"Setup Sandbox and Production Keycloak Realms Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11 Context and Problem Statement Keycloak enables separation of sets of users, groups, roles, etc. through Realms. Should we create separate realms by default, which ones? Decision Drivers Must reflect best practice configuration Segregate production from non-production workloads as much as possible Satisfy 'common' enterprise requirements Considered Options Leave single Master realm Create one production realm Create sandbox and production realms Decision Outcome We'll create sandbox and production keycloak realms. Given the existing inclusion of sandbox and production concepts through service mesh and ingress controllers, these realms make a simple and easy to understand separation of users that allows customers to also test and promote identity and access management policies in a similar way to application workloads. We've seen enterprises do this with okta and okta preview instances for example. Positive Consequences Simple, easy to understand Enables identity and access management separation Negative Consequences May be unnecessary for some organizations Links Keycloak Concetps","title":"Setup Sandbox and Production Keycloak Realms"},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#setup-sandbox-and-production-keycloak-realms","text":"Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11","title":"Setup Sandbox and Production Keycloak Realms"},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#context-and-problem-statement","text":"Keycloak enables separation of sets of users, groups, roles, etc. through Realms. Should we create separate realms by default, which ones?","title":"Context and Problem Statement"},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#decision-drivers","text":"Must reflect best practice configuration Segregate production from non-production workloads as much as possible Satisfy 'common' enterprise requirements","title":"Decision Drivers "},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#considered-options","text":"Leave single Master realm Create one production realm Create sandbox and production realms","title":"Considered Options"},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#decision-outcome","text":"We'll create sandbox and production keycloak realms. Given the existing inclusion of sandbox and production concepts through service mesh and ingress controllers, these realms make a simple and easy to understand separation of users that allows customers to also test and promote identity and access management policies in a similar way to application workloads. We've seen enterprises do this with okta and okta preview instances for example.","title":"Decision Outcome"},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#positive-consequences","text":"Simple, easy to understand Enables identity and access management separation","title":"Positive Consequences "},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#negative-consequences","text":"May be unnecessary for some organizations","title":"Negative Consequences "},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#links","text":"Keycloak Concetps","title":"Links "},{"location":"adr/0027-cert-manager-for-certificate-management/","text":"cert-manager for Certificate Management Status: accepted Deciders: @jam01 Date: 2020-11 Context and Problem Statement The platform will required TLS certificates to signed by well known CAs. What tool do we use for generate those certificates? Decision Drivers Automation, including rotation before expiration Cost Considered Options cert-manager Decision Outcome We'll use cert-manager as our certificate manager. cert-manager is the best known certificate manager for Kubernetes, it will automatically generate certificates as they're request by Ingress TLS configuration and will automatically re-generate them before they expire. For CA we'll use Let's Encrypt which is free. Positive Consequences Automatic generation and re-generation Negative Consequences Let's Encrypt limit of 50 certificates per week Links cert-manager Securing Ingress Resources ACME Issuers Let's Encrypt Docs","title":"cert-manager for Certificate Management"},{"location":"adr/0027-cert-manager-for-certificate-management/#cert-manager-for-certificate-management","text":"Status: accepted Deciders: @jam01 Date: 2020-11","title":"cert-manager for Certificate Management"},{"location":"adr/0027-cert-manager-for-certificate-management/#context-and-problem-statement","text":"The platform will required TLS certificates to signed by well known CAs. What tool do we use for generate those certificates?","title":"Context and Problem Statement"},{"location":"adr/0027-cert-manager-for-certificate-management/#decision-drivers","text":"Automation, including rotation before expiration Cost","title":"Decision Drivers "},{"location":"adr/0027-cert-manager-for-certificate-management/#considered-options","text":"cert-manager","title":"Considered Options"},{"location":"adr/0027-cert-manager-for-certificate-management/#decision-outcome","text":"We'll use cert-manager as our certificate manager. cert-manager is the best known certificate manager for Kubernetes, it will automatically generate certificates as they're request by Ingress TLS configuration and will automatically re-generate them before they expire. For CA we'll use Let's Encrypt which is free.","title":"Decision Outcome"},{"location":"adr/0027-cert-manager-for-certificate-management/#positive-consequences","text":"Automatic generation and re-generation","title":"Positive Consequences "},{"location":"adr/0027-cert-manager-for-certificate-management/#negative-consequences","text":"Let's Encrypt limit of 50 certificates per week","title":"Negative Consequences "},{"location":"adr/0027-cert-manager-for-certificate-management/#links","text":"cert-manager Securing Ingress Resources ACME Issuers Let's Encrypt Docs","title":"Links "},{"location":"adr/0028-use-markdown-architectural-decision-records/","text":"Use Markdown Architectural Decision Records Context and Problem Statement We want to record architectural decisions made in this project. Which format and structure should these records follow? Considered Options MADR 2.1.2 \u2013 The Markdown Architectural Decision Records Michael Nygard's template \u2013 The first incarnation of the term \"ADR\" Sustainable Architectural Decisions \u2013 The Y-Statements Other templates listed at https://github.com/joelparkerhenderson/architecture_decision_record Formless \u2013 No conventions for file format and structure Decision Outcome Chosen option: \"MADR 2.1.2\", because Implicit assumptions should be made explicit. Design documentation is important to enable people understanding the decisions later on. See also A rational design process: How and why to fake it . The MADR format is lean and fits our development style. The MADR structure is comprehensible and facilitates usage & maintenance. The MADR project is vivid. Version 2.1.2 is the latest one available when starting to document ADRs.","title":"Use Markdown Architectural Decision Records"},{"location":"adr/0028-use-markdown-architectural-decision-records/#use-markdown-architectural-decision-records","text":"","title":"Use Markdown Architectural Decision Records"},{"location":"adr/0028-use-markdown-architectural-decision-records/#context-and-problem-statement","text":"We want to record architectural decisions made in this project. Which format and structure should these records follow?","title":"Context and Problem Statement"},{"location":"adr/0028-use-markdown-architectural-decision-records/#considered-options","text":"MADR 2.1.2 \u2013 The Markdown Architectural Decision Records Michael Nygard's template \u2013 The first incarnation of the term \"ADR\" Sustainable Architectural Decisions \u2013 The Y-Statements Other templates listed at https://github.com/joelparkerhenderson/architecture_decision_record Formless \u2013 No conventions for file format and structure","title":"Considered Options"},{"location":"adr/0028-use-markdown-architectural-decision-records/#decision-outcome","text":"Chosen option: \"MADR 2.1.2\", because Implicit assumptions should be made explicit. Design documentation is important to enable people understanding the decisions later on. See also A rational design process: How and why to fake it . The MADR format is lean and fits our development style. The MADR structure is comprehensible and facilitates usage & maintenance. The MADR project is vivid. Version 2.1.2 is the latest one available when starting to document ADRs.","title":"Decision Outcome"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/","text":"Tavros as a Single Tenant Platform Status: accepted Deciders: @jam01 Date: 2020-11 Context and Problem Statement A single platform for an indefinite amount of tenants would allow us to provide Tavros in a cloud 'as-a-service' offering to our customers, instead of only as managed services. However, there is an unknown complexity and level of effort behind multi-tenancy. Decision Drivers Scope creep Speed of delivery Complexity Considered Options Build multi-tenancy from the ground up Automated single-tenant offered as PaaS Decision Outcome We'll build Tavros as a single tenant platform. The Ansible automation can be developed in a way that accommodates two modes of operation: Build a tavros cluster to be owned by the client and optionally operated by us, as was the original objective Build a tavros cluster to be owned and operated by us, transparently to the client. The second mode allows us to offer a PaaS while keeping the platform simple. Positive Consequences Explicit separation to other clusters Simpler to build and consequently faster to deliver Can still offer as PaaS Negative Consequences May complicate day-2 operations on multiple clusters at once","title":"Tavros as a Single Tenant Platform"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#tavros-as-a-single-tenant-platform","text":"Status: accepted Deciders: @jam01 Date: 2020-11","title":"Tavros as a Single Tenant Platform"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#context-and-problem-statement","text":"A single platform for an indefinite amount of tenants would allow us to provide Tavros in a cloud 'as-a-service' offering to our customers, instead of only as managed services. However, there is an unknown complexity and level of effort behind multi-tenancy.","title":"Context and Problem Statement"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#decision-drivers","text":"Scope creep Speed of delivery Complexity","title":"Decision Drivers "},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#considered-options","text":"Build multi-tenancy from the ground up Automated single-tenant offered as PaaS","title":"Considered Options"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#decision-outcome","text":"We'll build Tavros as a single tenant platform. The Ansible automation can be developed in a way that accommodates two modes of operation: Build a tavros cluster to be owned by the client and optionally operated by us, as was the original objective Build a tavros cluster to be owned and operated by us, transparently to the client. The second mode allows us to offer a PaaS while keeping the platform simple.","title":"Decision Outcome"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#positive-consequences","text":"Explicit separation to other clusters Simpler to build and consequently faster to deliver Can still offer as PaaS","title":"Positive Consequences "},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#negative-consequences","text":"May complicate day-2 operations on multiple clusters at once","title":"Negative Consequences "},{"location":"adr/template/","text":"[short title of solved problem and solution] Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 ] Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL] Context and Problem Statement [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] Decision Drivers [driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026 Considered Options [option 1] [option 2] [option 3] \u2026 Decision Outcome Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. Positive Consequences [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026 Negative Consequences [e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026 Pros and Cons of the Options [option 1] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 2] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 3] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 Links [Link type] [Link to ADR] \u2026","title":"[short title of solved problem and solution]"},{"location":"adr/template/#short-title-of-solved-problem-and-solution","text":"Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 ] Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL]","title":"[short title of solved problem and solution]"},{"location":"adr/template/#context-and-problem-statement","text":"[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]","title":"Context and Problem Statement"},{"location":"adr/template/#decision-drivers","text":"[driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026","title":"Decision Drivers "},{"location":"adr/template/#considered-options","text":"[option 1] [option 2] [option 3] \u2026","title":"Considered Options"},{"location":"adr/template/#decision-outcome","text":"Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].","title":"Decision Outcome"},{"location":"adr/template/#positive-consequences","text":"[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026","title":"Positive Consequences "},{"location":"adr/template/#negative-consequences","text":"[e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026","title":"Negative Consequences "},{"location":"adr/template/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options "},{"location":"adr/template/#option-1","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 1]"},{"location":"adr/template/#option-2","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 2]"},{"location":"adr/template/#option-3","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 3]"},{"location":"adr/template/#links","text":"[Link type] [Link to ADR] \u2026","title":"Links "}]}