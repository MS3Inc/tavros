{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tavros \u00b6 Tavros is a cost-effective, cloud-native, and modular integration platform composed of best-of-breed, and seamlessly integrated open-source components. Ansible Collection \u00b6 The objective of the ms3_inc.tavros Ansible Collection is to provide the necessary Ansible Playbooks to configure, provision, and manage the Tavros Kubernetes Cluster and supported components. Provision Playbook \u00b6 The provision playbook provisions a Kubernetes cluster and configures Tavros's platform components, application environments, etc. All of the components are configurable through Ansible variables or the default configuration can be chosen. See the provision playbook's documentation for more information. Supported Platform Components \u00b6 Concern Component Version Platform GitOps Flux v2 0.10.0 Platform GitOps Sealed Secrets 0.15.0 API Gateway and Manager Kong 2.3.3 API Portal Kong Enterprise Edition 2.3.3 Service Mesh Kuma 1.2.0 Identity and Access Management Keycloak 12.0.4 Artifact Management Nexus Repository Manager 3.28.1 Continuous Delivery Jenkins 2.277.4 Observability Elastic Cloud 7.13.4 Observability Jaeger 1.22.0 Static Code Qualitative Analysis Sonarqube 8.5 Roadmap \u00b6 The Tavros team will maintain an up to date roadmap for major and minor releases through its Milestones . For items that are not yet targeting a milestone, you can see our Backlog Architectural Decision Log \u00b6 This project documents significant architectural decisions in MADR, a lightweight format for recording architectural decisions in Markdown. See our Architectural Decision Log .","title":"About"},{"location":"#tavros","text":"Tavros is a cost-effective, cloud-native, and modular integration platform composed of best-of-breed, and seamlessly integrated open-source components.","title":"Tavros"},{"location":"#ansible-collection","text":"The objective of the ms3_inc.tavros Ansible Collection is to provide the necessary Ansible Playbooks to configure, provision, and manage the Tavros Kubernetes Cluster and supported components.","title":"Ansible Collection"},{"location":"#provision-playbook","text":"The provision playbook provisions a Kubernetes cluster and configures Tavros's platform components, application environments, etc. All of the components are configurable through Ansible variables or the default configuration can be chosen. See the provision playbook's documentation for more information.","title":"Provision Playbook"},{"location":"#supported-platform-components","text":"Concern Component Version Platform GitOps Flux v2 0.10.0 Platform GitOps Sealed Secrets 0.15.0 API Gateway and Manager Kong 2.3.3 API Portal Kong Enterprise Edition 2.3.3 Service Mesh Kuma 1.2.0 Identity and Access Management Keycloak 12.0.4 Artifact Management Nexus Repository Manager 3.28.1 Continuous Delivery Jenkins 2.277.4 Observability Elastic Cloud 7.13.4 Observability Jaeger 1.22.0 Static Code Qualitative Analysis Sonarqube 8.5","title":"Supported Platform Components"},{"location":"#roadmap","text":"The Tavros team will maintain an up to date roadmap for major and minor releases through its Milestones . For items that are not yet targeting a milestone, you can see our Backlog","title":"Roadmap"},{"location":"#architectural-decision-log","text":"This project documents significant architectural decisions in MADR, a lightweight format for recording architectural decisions in Markdown. See our Architectural Decision Log .","title":"Architectural Decision Log"},{"location":"adr/","text":"Architectural Decision Records \u00b6 This log lists the architectural decisions for Tavros. ADR-0000 - Prefer Proven FOSS Components with Optional Support for Licensed Derivatives ADR-0001 - Apache Camel as the Default Integration Framework ADR-0002 - Spring Boot as the Base Application Framework ADR-0003 - DataSonnet as the Default Data Transformation Language ADR-0004 - OpenTracing for In-Process Tracing API ADR-0005 - Kubernetes as the Computing Platform ADR-0006 - Kops to Provision a Kubernetes Cluster ADR-0007 - Flux to Provide Platform GitOps ADR-0008 - Kubeseal to Securely Manage Secrets in GitOps ADR-0009 - Keycloak for Indetity and Access Management ADR-0010 - Kong as Kubernetes Ingress and API gateway ADR-0011 - PostgreSQL as the Platform's Default Database ADR-0012 - Gitea for a Lightweight Git Server ADR-0013 - Kuma for Service Mesh ADR-0014 - Jenkins for Continuous Integration ADR-0015 - Sonarqube for Application Static Code Analysis ADR-0016 - Elastic Cloud for Observability Data Aggregation and Visualization ADR-0017 - Jaeger for Tracing with Elasticsearch Backend ADR-0018 - Nexus Repository Manager for Artifact Management ADR-0019 - Prefer Daemonsets Over Sidecars ADR-0020 - Spring Cloud Config for Application Configuration Management ADR-0021 - Prefer Kong Enterprise Edition ADR-0022 - Use Ansible as the Provisioning Engine ADR-0023 - Helm and Operators for Component Installation and Management ADR-0024 - Use Ansible Collection to Structure and Package Ansible Code ADR-0025 - Setup Sandbox and Production Kuma Meshes and Kong Ingress Controllers ADR-0026 - Setup Sandbox and Production Keycloak Realms ADR-0027 - cert-manager for Certificate Management ADR-0028 - Use Markdown Architectural Decision Records ADR-0029 - Tavros as a Single Tenant Platform For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/ . General information about architectural decision records is available at https://adr.github.io/ .","title":"ADR"},{"location":"adr/#architectural-decision-records","text":"This log lists the architectural decisions for Tavros. ADR-0000 - Prefer Proven FOSS Components with Optional Support for Licensed Derivatives ADR-0001 - Apache Camel as the Default Integration Framework ADR-0002 - Spring Boot as the Base Application Framework ADR-0003 - DataSonnet as the Default Data Transformation Language ADR-0004 - OpenTracing for In-Process Tracing API ADR-0005 - Kubernetes as the Computing Platform ADR-0006 - Kops to Provision a Kubernetes Cluster ADR-0007 - Flux to Provide Platform GitOps ADR-0008 - Kubeseal to Securely Manage Secrets in GitOps ADR-0009 - Keycloak for Indetity and Access Management ADR-0010 - Kong as Kubernetes Ingress and API gateway ADR-0011 - PostgreSQL as the Platform's Default Database ADR-0012 - Gitea for a Lightweight Git Server ADR-0013 - Kuma for Service Mesh ADR-0014 - Jenkins for Continuous Integration ADR-0015 - Sonarqube for Application Static Code Analysis ADR-0016 - Elastic Cloud for Observability Data Aggregation and Visualization ADR-0017 - Jaeger for Tracing with Elasticsearch Backend ADR-0018 - Nexus Repository Manager for Artifact Management ADR-0019 - Prefer Daemonsets Over Sidecars ADR-0020 - Spring Cloud Config for Application Configuration Management ADR-0021 - Prefer Kong Enterprise Edition ADR-0022 - Use Ansible as the Provisioning Engine ADR-0023 - Helm and Operators for Component Installation and Management ADR-0024 - Use Ansible Collection to Structure and Package Ansible Code ADR-0025 - Setup Sandbox and Production Kuma Meshes and Kong Ingress Controllers ADR-0026 - Setup Sandbox and Production Keycloak Realms ADR-0027 - cert-manager for Certificate Management ADR-0028 - Use Markdown Architectural Decision Records ADR-0029 - Tavros as a Single Tenant Platform For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/ . General information about architectural decision records is available at https://adr.github.io/ .","title":"Architectural Decision Records"},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/","text":"Prefer Proven FOSS Components with Optional Support for Licensed Derivatives \u00b6 Status: accepted Deciders: @k2merlinsix, @jam01 Date: 2020-08 Context and Problem Statement \u00b6 Should we include licensed or non-FOSS components in the Tavros platform for a given component or component's feature? Decision Drivers \u00b6 Total cost of ownership Providing the necessary features for a competitive product Decision Outcome \u00b6 We will prioritize FOSS components, while providing optional support for paid or licensed derivatives, e.g.: Elastic Stack Open Source vs Enterprise subscription, Apache Camel vs JBoss Fuse, etc. Ultimately total cost of ownership will be the deciding factor for our customers when comparing to established alternatives. There are many FOSS components that address different concerns that we can incorporate and still provide a best-in-class platform. Negative Consequences \u00b6 High number of components to configure and integrate","title":"Prefer Proven FOSS Components with Optional Support for Licensed Derivatives"},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/#prefer-proven-foss-components-with-optional-support-for-licensed-derivatives","text":"Status: accepted Deciders: @k2merlinsix, @jam01 Date: 2020-08","title":"Prefer Proven FOSS Components with Optional Support for Licensed Derivatives"},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/#context-and-problem-statement","text":"Should we include licensed or non-FOSS components in the Tavros platform for a given component or component's feature?","title":"Context and Problem Statement"},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/#decision-drivers","text":"Total cost of ownership Providing the necessary features for a competitive product","title":"Decision Drivers "},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/#decision-outcome","text":"We will prioritize FOSS components, while providing optional support for paid or licensed derivatives, e.g.: Elastic Stack Open Source vs Enterprise subscription, Apache Camel vs JBoss Fuse, etc. Ultimately total cost of ownership will be the deciding factor for our customers when comparing to established alternatives. There are many FOSS components that address different concerns that we can incorporate and still provide a best-in-class platform.","title":"Decision Outcome"},{"location":"adr/0000-prefer-proven-foss-components-with-optional-support-for-licensed-derivatives/#negative-consequences","text":"High number of components to configure and integrate","title":"Negative Consequences "},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/","text":"Apache Camel as the Default Integration Framework \u00b6 Status: accepted Deciders: @k2merlinsix, @jam01, @mnorton Date: 2020-08 Context and Problem Statement \u00b6 What should be our main/default software framework for integration services? Decision Drivers \u00b6 Modern framework Low learning curve Maturity Solid documentation and community Cost Considered Options \u00b6 Spring Boot Spring Integration Apache Camel Python Flask Alpakka Go Decision Outcome \u00b6 We'll use and extend Apache Camel as our default integration framework. Given the proficiency with Java and Enterprise Integration Patterns within our company and the industry in general, Apache Camel gives our customers a straight forward and low cost migration path from existing implementations. Positive Consequences \u00b6 Existing proficiency with Java and EIP Enables re-use of Maven based artifacts, e.g.: custom exceptions, domain classes, logging layouts, CI/CD pipelines Lower barrier for our customers finding professional resources Links \u00b6 Enterprise Integration Patterns Apache Camel Spring Boot Spring Integration Alpakka","title":"Apache Camel as the Default Integration Framework"},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#apache-camel-as-the-default-integration-framework","text":"Status: accepted Deciders: @k2merlinsix, @jam01, @mnorton Date: 2020-08","title":"Apache Camel as the Default Integration Framework"},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#context-and-problem-statement","text":"What should be our main/default software framework for integration services?","title":"Context and Problem Statement"},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#decision-drivers","text":"Modern framework Low learning curve Maturity Solid documentation and community Cost","title":"Decision Drivers "},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#considered-options","text":"Spring Boot Spring Integration Apache Camel Python Flask Alpakka Go","title":"Considered Options"},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#decision-outcome","text":"We'll use and extend Apache Camel as our default integration framework. Given the proficiency with Java and Enterprise Integration Patterns within our company and the industry in general, Apache Camel gives our customers a straight forward and low cost migration path from existing implementations.","title":"Decision Outcome"},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#positive-consequences","text":"Existing proficiency with Java and EIP Enables re-use of Maven based artifacts, e.g.: custom exceptions, domain classes, logging layouts, CI/CD pipelines Lower barrier for our customers finding professional resources","title":"Positive Consequences "},{"location":"adr/0001-apache-camel-as-the-default-integration-framework/#links","text":"Enterprise Integration Patterns Apache Camel Spring Boot Spring Integration Alpakka","title":"Links "},{"location":"adr/0002-spring-boot-as-the-base-application-framework/","text":"Spring Boot as the Base Application Framework \u00b6 Status: accepted Deciders: @k2merlinsix, @jam01, @mnorton Date: 2020-08 Context and Problem Statement \u00b6 Should we add a base application framework to Apache Camel such as Spring Boot or Quarkus? Decision Drivers \u00b6 Speed up development Maturity Solid documentation and community Considered Options \u00b6 Spring Boot Quarkus Decision Outcome \u00b6 We'll use Spring Boot as the base application framework to Apache Camel. We found that Spring Boot's auto-configuration features greatly speed up development time and it is well known and documented Positive Consequences \u00b6 Existing proficiency with Spring Boot as part of our internal bootcamp Access to a bigger ecosystem, e.g.: Spring Cloud Config Links \u00b6 Spring Boot Quarkus","title":"Spring Boot as the Base Application Framework"},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#spring-boot-as-the-base-application-framework","text":"Status: accepted Deciders: @k2merlinsix, @jam01, @mnorton Date: 2020-08","title":"Spring Boot as the Base Application Framework"},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#context-and-problem-statement","text":"Should we add a base application framework to Apache Camel such as Spring Boot or Quarkus?","title":"Context and Problem Statement"},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#decision-drivers","text":"Speed up development Maturity Solid documentation and community","title":"Decision Drivers "},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#considered-options","text":"Spring Boot Quarkus","title":"Considered Options"},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#decision-outcome","text":"We'll use Spring Boot as the base application framework to Apache Camel. We found that Spring Boot's auto-configuration features greatly speed up development time and it is well known and documented","title":"Decision Outcome"},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#positive-consequences","text":"Existing proficiency with Spring Boot as part of our internal bootcamp Access to a bigger ecosystem, e.g.: Spring Cloud Config","title":"Positive Consequences "},{"location":"adr/0002-spring-boot-as-the-base-application-framework/#links","text":"Spring Boot Quarkus","title":"Links "},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/","text":"DataSonnet as the Default Data Transformation Language \u00b6 Status: accepted Deciders: @k2merlinsix, @jam01, @JakeMHughes Date: 2020-08 Context and Problem Statement \u00b6 Does DataSonnet, or can we make it, meet functional and performance requirements for modern integration workloads? Decision Drivers \u00b6 Provide the necessary transformation functions Performance Decision Outcome \u00b6 We'll extend DataSonnet and use it as our default data transformation language. Extending ModusBox' work on top of DataBrick's sjsonnet project we delivered a feature-full and very performant language. Positive Consequences \u00b6 Joint ownership with ModusBox offers great visibility to our company and offerings Apache 2.0 License opens possibilities of adoption by the larger community or existing projects in the same space Negative Consequences \u00b6 Lead time to delivery of the next major release Responsibility of maintaining and stewarding the project with the community Links \u00b6 DataSonnet Mapper Project DataSonnet Jsonnet sjsonnet Project","title":"DataSonnet as the Default Data Transformation Language"},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#datasonnet-as-the-default-data-transformation-language","text":"Status: accepted Deciders: @k2merlinsix, @jam01, @JakeMHughes Date: 2020-08","title":"DataSonnet as the Default Data Transformation Language"},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#context-and-problem-statement","text":"Does DataSonnet, or can we make it, meet functional and performance requirements for modern integration workloads?","title":"Context and Problem Statement"},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#decision-drivers","text":"Provide the necessary transformation functions Performance","title":"Decision Drivers "},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#decision-outcome","text":"We'll extend DataSonnet and use it as our default data transformation language. Extending ModusBox' work on top of DataBrick's sjsonnet project we delivered a feature-full and very performant language.","title":"Decision Outcome"},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#positive-consequences","text":"Joint ownership with ModusBox offers great visibility to our company and offerings Apache 2.0 License opens possibilities of adoption by the larger community or existing projects in the same space","title":"Positive Consequences "},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#negative-consequences","text":"Lead time to delivery of the next major release Responsibility of maintaining and stewarding the project with the community","title":"Negative Consequences "},{"location":"adr/0003-datasonnet-as-the-default-data-transformation-language/#links","text":"DataSonnet Mapper Project DataSonnet Jsonnet sjsonnet Project","title":"Links "},{"location":"adr/0004-opentracing-for-in-process-tracing-api/","text":"OpenTracing for In-Process Tracing API \u00b6 Status: accepted Deciders: @jam01 Date: 2020-08 Context and Problem Statement \u00b6 Which in-process tracing API should we use, OpenTracing or the new project OpenTelemetry? Decision Drivers \u00b6 Existing library integrations Considered Options \u00b6 OpenTracing OpenTelemetry Decision Outcome \u00b6 We'll use OpenTracing for in-process tracing API. Having contributed to the OpenTracing project, and refactoring the camel-opentracing component significantly, OpenTracing provides the most functionality now while OpenTelemetry's design stabilizes. Negative Consequences \u00b6 Will need to migrate to OpenTelemetry as it eventually deprecates OpenTracing Links \u00b6 OpenTracing OpenTelemetry","title":"OpenTracing for In-Process Tracing API"},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#opentracing-for-in-process-tracing-api","text":"Status: accepted Deciders: @jam01 Date: 2020-08","title":"OpenTracing for In-Process Tracing API"},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#context-and-problem-statement","text":"Which in-process tracing API should we use, OpenTracing or the new project OpenTelemetry?","title":"Context and Problem Statement"},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#decision-drivers","text":"Existing library integrations","title":"Decision Drivers "},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#considered-options","text":"OpenTracing OpenTelemetry","title":"Considered Options"},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#decision-outcome","text":"We'll use OpenTracing for in-process tracing API. Having contributed to the OpenTracing project, and refactoring the camel-opentracing component significantly, OpenTracing provides the most functionality now while OpenTelemetry's design stabilizes.","title":"Decision Outcome"},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#negative-consequences","text":"Will need to migrate to OpenTelemetry as it eventually deprecates OpenTracing","title":"Negative Consequences "},{"location":"adr/0004-opentracing-for-in-process-tracing-api/#links","text":"OpenTracing OpenTelemetry","title":"Links "},{"location":"adr/0005-kubernetes-as-the-computing-platform/","text":"Kubernetes as the Computing Platform \u00b6 Status: accepted Deciders: @k2merlinsix, @jam01, Mohammad Naeem Date: 2020-08 Context and Problem Statement \u00b6 What should be our base platform? Decision Drivers \u00b6 Existing tooling and integrations to speed up delivery Decision Outcome \u00b6 We'll use Kubernetes as Tavros's computing platform. The adoption and support for Kubernetes has been evident for some time, most importantly tools like Kops, Helm, Operators, and Ansible's Kubernetes modules, make a great base for us to focus on delivering our platform.","title":"Kubernetes as the Computing Platform"},{"location":"adr/0005-kubernetes-as-the-computing-platform/#kubernetes-as-the-computing-platform","text":"Status: accepted Deciders: @k2merlinsix, @jam01, Mohammad Naeem Date: 2020-08","title":"Kubernetes as the Computing Platform"},{"location":"adr/0005-kubernetes-as-the-computing-platform/#context-and-problem-statement","text":"What should be our base platform?","title":"Context and Problem Statement"},{"location":"adr/0005-kubernetes-as-the-computing-platform/#decision-drivers","text":"Existing tooling and integrations to speed up delivery","title":"Decision Drivers "},{"location":"adr/0005-kubernetes-as-the-computing-platform/#decision-outcome","text":"We'll use Kubernetes as Tavros's computing platform. The adoption and support for Kubernetes has been evident for some time, most importantly tools like Kops, Helm, Operators, and Ansible's Kubernetes modules, make a great base for us to focus on delivering our platform.","title":"Decision Outcome"},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/","text":"Kops to Provision a Kubernetes Cluster \u00b6 Status: accepted Deciders: Mohammad Naeem Date: 2020-08 Context and Problem Statement \u00b6 In order to automate the provisioning of the Kubernetes cluster, what tools should we use? Decision Outcome \u00b6 We'll utilize Kops to provision Kubernetes clusters. Positive Consequences \u00b6 Support for multiple cloud providers Links \u00b6 Kops","title":"Kops to Provision a Kubernetes Cluster"},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/#kops-to-provision-a-kubernetes-cluster","text":"Status: accepted Deciders: Mohammad Naeem Date: 2020-08","title":"Kops to Provision a Kubernetes Cluster"},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/#context-and-problem-statement","text":"In order to automate the provisioning of the Kubernetes cluster, what tools should we use?","title":"Context and Problem Statement"},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/#decision-outcome","text":"We'll utilize Kops to provision Kubernetes clusters.","title":"Decision Outcome"},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/#positive-consequences","text":"Support for multiple cloud providers","title":"Positive Consequences "},{"location":"adr/0006-kops-to-provision-a-kubernetes-cluster/#links","text":"Kops","title":"Links "},{"location":"adr/0007-flux-to-provide-platform-gitops/","text":"Flux v2 Toolkit to Provide Platform GitOps \u00b6 Status: accepted Deciders: Mohammad Naeem, @jam01, @rmccright-ms3 Date: 2020-09 Context and Problem Statement \u00b6 We want to enable continuous delivery of platform and application workloads in a GitOps way. What tools should we use? Decision Drivers \u00b6 Simplicity Integration with tools like Helm and Kustomize Considered Options \u00b6 Flux Flux v2 GitOps Toolkit Argo CD Decision Outcome \u00b6 We'll use Flux v2 GitOps Toolkit to enable continuous delivery of the platform and application workloads. Having had experience with Flux v1 internally, along with the newer features of v2 while still maintaining a simple workflow, it's the more appropriate tool. Positive Consequences \u00b6 Flux v2 Custom Resource Definitions make it easy to utilize Flux Helm functionality before the source Git repository is up Alert features through integrations like Slack Negative Consequences \u00b6 Have to be careful with the 'chicken and egg problem' between Flux managing the platform, and provisioning platform components through Flux Links \u00b6 GitOps Guide Flux v2 GitOps Toolkit Argo CD","title":"Flux v2 Toolkit to Provide Platform GitOps"},{"location":"adr/0007-flux-to-provide-platform-gitops/#flux-v2-toolkit-to-provide-platform-gitops","text":"Status: accepted Deciders: Mohammad Naeem, @jam01, @rmccright-ms3 Date: 2020-09","title":"Flux v2 Toolkit to Provide Platform GitOps"},{"location":"adr/0007-flux-to-provide-platform-gitops/#context-and-problem-statement","text":"We want to enable continuous delivery of platform and application workloads in a GitOps way. What tools should we use?","title":"Context and Problem Statement"},{"location":"adr/0007-flux-to-provide-platform-gitops/#decision-drivers","text":"Simplicity Integration with tools like Helm and Kustomize","title":"Decision Drivers "},{"location":"adr/0007-flux-to-provide-platform-gitops/#considered-options","text":"Flux Flux v2 GitOps Toolkit Argo CD","title":"Considered Options"},{"location":"adr/0007-flux-to-provide-platform-gitops/#decision-outcome","text":"We'll use Flux v2 GitOps Toolkit to enable continuous delivery of the platform and application workloads. Having had experience with Flux v1 internally, along with the newer features of v2 while still maintaining a simple workflow, it's the more appropriate tool.","title":"Decision Outcome"},{"location":"adr/0007-flux-to-provide-platform-gitops/#positive-consequences","text":"Flux v2 Custom Resource Definitions make it easy to utilize Flux Helm functionality before the source Git repository is up Alert features through integrations like Slack","title":"Positive Consequences "},{"location":"adr/0007-flux-to-provide-platform-gitops/#negative-consequences","text":"Have to be careful with the 'chicken and egg problem' between Flux managing the platform, and provisioning platform components through Flux","title":"Negative Consequences "},{"location":"adr/0007-flux-to-provide-platform-gitops/#links","text":"GitOps Guide Flux v2 GitOps Toolkit Argo CD","title":"Links "},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/","text":"Kubeseal to Securely Manage Secrets in GitOps \u00b6 Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-09 Context and Problem Statement \u00b6 In order to store secrets safely in a public or private Git repository, what tool do we use? Decision Drivers \u00b6 Simplicity Integration with Flux Decision Outcome \u00b6 We'll use Bitnami's Sealed Secrets controller to securely manage secrets in GitOps. The sealed secrets can be decrypted only by the controller running in your cluster and nobody else can obtain the original secret, even if they have access to the Git repository. Links \u00b6 Flux Sealed Secrets recommendation Sealed Secrets project","title":"Kubeseal to Securely Manage Secrets in GitOps"},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/#kubeseal-to-securely-manage-secrets-in-gitops","text":"Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-09","title":"Kubeseal to Securely Manage Secrets in GitOps"},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/#context-and-problem-statement","text":"In order to store secrets safely in a public or private Git repository, what tool do we use?","title":"Context and Problem Statement"},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/#decision-drivers","text":"Simplicity Integration with Flux","title":"Decision Drivers "},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/#decision-outcome","text":"We'll use Bitnami's Sealed Secrets controller to securely manage secrets in GitOps. The sealed secrets can be decrypted only by the controller running in your cluster and nobody else can obtain the original secret, even if they have access to the Git repository.","title":"Decision Outcome"},{"location":"adr/0008-kubeseal-to-securely-manage-secrets-in-gitops/#links","text":"Flux Sealed Secrets recommendation Sealed Secrets project","title":"Links "},{"location":"adr/0009-keycloak-for-indetity-and-access-management/","text":"Keycloak for Identity and Access Management \u00b6 Status: accepted Deciders: Mohammad Naeem Date: 2020-08 Context and Problem Statement \u00b6 To provide user identity and access management, what component do we use? Decision Outcome \u00b6 We'll use Keycloak for identity and access management. Links \u00b6 Keycloak","title":"Keycloak for Identity and Access Management"},{"location":"adr/0009-keycloak-for-indetity-and-access-management/#keycloak-for-identity-and-access-management","text":"Status: accepted Deciders: Mohammad Naeem Date: 2020-08","title":"Keycloak for Identity and Access Management"},{"location":"adr/0009-keycloak-for-indetity-and-access-management/#context-and-problem-statement","text":"To provide user identity and access management, what component do we use?","title":"Context and Problem Statement"},{"location":"adr/0009-keycloak-for-indetity-and-access-management/#decision-outcome","text":"We'll use Keycloak for identity and access management.","title":"Decision Outcome"},{"location":"adr/0009-keycloak-for-indetity-and-access-management/#links","text":"Keycloak","title":"Links "},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/","text":"Kong as Kubernetes Ingress Controller and API Gateway \u00b6 Status: accepted Deciders: @k2merlinsix Date: 2020-08 Context and Problem Statement \u00b6 Which component should serve as our Kubernetes Ingress Controller, to do load balancing and proxying? Which component should serve as our API Gateway? How easy is it to tie into the application networking of the Ingress Controller? Decision Drivers \u00b6 Network performance Feature set of API Gateway Considered Options \u00b6 NGINX Kong Apigee KrakenD Decision Outcome \u00b6 Kong will be our Kubernetes Ingress Controller and API Gateway as well. Having a single component perform both functions should make it easier to maintain and more performant. Positive Consequences \u00b6 Enterprise Edition pricing is more cost effective than some alternatives Negative Consequences \u00b6 Lua as the main language for developing plugins Links \u00b6 Kong NGINX Apigee KrakenD","title":"Kong as Kubernetes Ingress Controller and API Gateway"},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#kong-as-kubernetes-ingress-controller-and-api-gateway","text":"Status: accepted Deciders: @k2merlinsix Date: 2020-08","title":"Kong as Kubernetes Ingress Controller and API Gateway"},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#context-and-problem-statement","text":"Which component should serve as our Kubernetes Ingress Controller, to do load balancing and proxying? Which component should serve as our API Gateway? How easy is it to tie into the application networking of the Ingress Controller?","title":"Context and Problem Statement"},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#decision-drivers","text":"Network performance Feature set of API Gateway","title":"Decision Drivers "},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#considered-options","text":"NGINX Kong Apigee KrakenD","title":"Considered Options"},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#decision-outcome","text":"Kong will be our Kubernetes Ingress Controller and API Gateway as well. Having a single component perform both functions should make it easier to maintain and more performant.","title":"Decision Outcome"},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#positive-consequences","text":"Enterprise Edition pricing is more cost effective than some alternatives","title":"Positive Consequences "},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#negative-consequences","text":"Lua as the main language for developing plugins","title":"Negative Consequences "},{"location":"adr/0010-kong-as-kubernetes-ingress-and-api-gateway/#links","text":"Kong NGINX Apigee KrakenD","title":"Links "},{"location":"adr/0011-postgresql-as-the-platform%27s-default-database/","text":"PostgreSQL as the Platform's Default Database \u00b6 Status: accepted Deciders: Mohammad Naeem Date: 2020-09 Context and Problem Statement \u00b6 Some components require a PostgreSQL database, how do we accommodate each one and future components? Decision Outcome \u00b6 We'll use PostgreSQL as the platform's default database. Since Gitea and Keycloak require a PostgreSQL database we'll setup a single server and dynamically create what's needed for each component that uses PostgreSQL. If it's an option to use PostgreSQL for future components, we'll use that. Positive Consequences \u00b6 Single instance to backup, and restore in case of a disaster recovery","title":"PostgreSQL as the Platform's Default Database"},{"location":"adr/0011-postgresql-as-the-platform%27s-default-database/#postgresql-as-the-platforms-default-database","text":"Status: accepted Deciders: Mohammad Naeem Date: 2020-09","title":"PostgreSQL as the Platform's Default Database"},{"location":"adr/0011-postgresql-as-the-platform%27s-default-database/#context-and-problem-statement","text":"Some components require a PostgreSQL database, how do we accommodate each one and future components?","title":"Context and Problem Statement"},{"location":"adr/0011-postgresql-as-the-platform%27s-default-database/#decision-outcome","text":"We'll use PostgreSQL as the platform's default database. Since Gitea and Keycloak require a PostgreSQL database we'll setup a single server and dynamically create what's needed for each component that uses PostgreSQL. If it's an option to use PostgreSQL for future components, we'll use that.","title":"Decision Outcome"},{"location":"adr/0011-postgresql-as-the-platform%27s-default-database/#positive-consequences","text":"Single instance to backup, and restore in case of a disaster recovery","title":"Positive Consequences "},{"location":"adr/0012-gitea-for-a-lightweight-git-server/","text":"Gitea for a Lightweight Git Server \u00b6 Status: accepted Deciders: Mohammad Naeem, @jam01 Date: 2020-09 Context and Problem Statement \u00b6 Given that Flux requires a Git repository to provide GitOps, and that our CI/CD pipelines will require the same for continuous delivery, what component should be our default/bundled Git server? Decision Drivers \u00b6 Lightweight as we expect a number of customers will already have a Git saas Easy to integrate Considered Options \u00b6 GitLab Gitea Decision Outcome \u00b6 We'll use Gitea as our bundled and lightweight Git server. Gitea is more lightweight than the alternatives, and has the necessary APIs for us to integrate with. Negative Consequences \u00b6 No out of the box integration with Flux Links \u00b6 Gitea GitLab","title":"Gitea for a Lightweight Git Server"},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#gitea-for-a-lightweight-git-server","text":"Status: accepted Deciders: Mohammad Naeem, @jam01 Date: 2020-09","title":"Gitea for a Lightweight Git Server"},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#context-and-problem-statement","text":"Given that Flux requires a Git repository to provide GitOps, and that our CI/CD pipelines will require the same for continuous delivery, what component should be our default/bundled Git server?","title":"Context and Problem Statement"},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#decision-drivers","text":"Lightweight as we expect a number of customers will already have a Git saas Easy to integrate","title":"Decision Drivers "},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#considered-options","text":"GitLab Gitea","title":"Considered Options"},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#decision-outcome","text":"We'll use Gitea as our bundled and lightweight Git server. Gitea is more lightweight than the alternatives, and has the necessary APIs for us to integrate with.","title":"Decision Outcome"},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#negative-consequences","text":"No out of the box integration with Flux","title":"Negative Consequences "},{"location":"adr/0012-gitea-for-a-lightweight-git-server/#links","text":"Gitea GitLab","title":"Links "},{"location":"adr/0013-kuma-for-service-mesh/","text":"Kuma for Service Mesh \u00b6 Status: accepted Deciders: @k2merlinsix Date: 2020-08 Context and Problem Statement \u00b6 Service meshes helps to address cross cutting concerns so that application developers don't have to, what service mesh component should we use? Decision Drivers \u00b6 Performance Feature set Considered Options \u00b6 Kuma Istio Decision Outcome \u00b6 We'll use Kuma for service mesh. Positive Consequences \u00b6 Seamless integration with Kong Links \u00b6 Kuma","title":"Kuma for Service Mesh"},{"location":"adr/0013-kuma-for-service-mesh/#kuma-for-service-mesh","text":"Status: accepted Deciders: @k2merlinsix Date: 2020-08","title":"Kuma for Service Mesh"},{"location":"adr/0013-kuma-for-service-mesh/#context-and-problem-statement","text":"Service meshes helps to address cross cutting concerns so that application developers don't have to, what service mesh component should we use?","title":"Context and Problem Statement"},{"location":"adr/0013-kuma-for-service-mesh/#decision-drivers","text":"Performance Feature set","title":"Decision Drivers "},{"location":"adr/0013-kuma-for-service-mesh/#considered-options","text":"Kuma Istio","title":"Considered Options"},{"location":"adr/0013-kuma-for-service-mesh/#decision-outcome","text":"We'll use Kuma for service mesh.","title":"Decision Outcome"},{"location":"adr/0013-kuma-for-service-mesh/#positive-consequences","text":"Seamless integration with Kong","title":"Positive Consequences "},{"location":"adr/0013-kuma-for-service-mesh/#links","text":"Kuma","title":"Links "},{"location":"adr/0014-jenkins-for-continuous-integration/","text":"Jenkins for Continuous Integration \u00b6 Status: accepted Deciders: @jam01 Date: 2020-09 Context and Problem Statement \u00b6 We'll be using Flux for continuous deployment, leaving us to choose a component for continuous delivery, i.e.: building and running tests against artifacts before deploying. Decision Drivers \u00b6 Existing proficiency and pipelines Simplicity Considered Options \u00b6 Jenkins Jenkins X Tekton Decision Outcome \u00b6 We'll use Jenkins as our CI component. Since we're using Flux for deployment we don't need too Kubernetes specific features like Jenkins X has, and Jenkins can support for complex pipelines than Tekton. Positive Consequences \u00b6 Can re-use existing pipelines with little modification Negative Consequences \u00b6 Less 'modern' than other options Links \u00b6 Jenkins X Jenkins Tekton Project","title":"Jenkins for Continuous Integration"},{"location":"adr/0014-jenkins-for-continuous-integration/#jenkins-for-continuous-integration","text":"Status: accepted Deciders: @jam01 Date: 2020-09","title":"Jenkins for Continuous Integration"},{"location":"adr/0014-jenkins-for-continuous-integration/#context-and-problem-statement","text":"We'll be using Flux for continuous deployment, leaving us to choose a component for continuous delivery, i.e.: building and running tests against artifacts before deploying.","title":"Context and Problem Statement"},{"location":"adr/0014-jenkins-for-continuous-integration/#decision-drivers","text":"Existing proficiency and pipelines Simplicity","title":"Decision Drivers "},{"location":"adr/0014-jenkins-for-continuous-integration/#considered-options","text":"Jenkins Jenkins X Tekton","title":"Considered Options"},{"location":"adr/0014-jenkins-for-continuous-integration/#decision-outcome","text":"We'll use Jenkins as our CI component. Since we're using Flux for deployment we don't need too Kubernetes specific features like Jenkins X has, and Jenkins can support for complex pipelines than Tekton.","title":"Decision Outcome"},{"location":"adr/0014-jenkins-for-continuous-integration/#positive-consequences","text":"Can re-use existing pipelines with little modification","title":"Positive Consequences "},{"location":"adr/0014-jenkins-for-continuous-integration/#negative-consequences","text":"Less 'modern' than other options","title":"Negative Consequences "},{"location":"adr/0014-jenkins-for-continuous-integration/#links","text":"Jenkins X Jenkins Tekton Project","title":"Links "},{"location":"adr/0015-sonarqube-for-application-static-code-analysis/","text":"Sonarqube for Application Static Code Analysis \u00b6 Status: accepted Deciders: @jam01 Date: 2020-10 Context and Problem Statement \u00b6 We understand the value of automated code qualitative analysis and that it's often an afterthought. Should we bundle a static code analysis component? Decision Outcome \u00b6 We'll use Sonarqube for application static code analysis. As part of our effort to offer a ready best-practices platform we'll setup Sonarqube and tie it into template or default CI/CD pipelines. Links \u00b6 Sonarqube","title":"Sonarqube for Application Static Code Analysis"},{"location":"adr/0015-sonarqube-for-application-static-code-analysis/#sonarqube-for-application-static-code-analysis","text":"Status: accepted Deciders: @jam01 Date: 2020-10","title":"Sonarqube for Application Static Code Analysis"},{"location":"adr/0015-sonarqube-for-application-static-code-analysis/#context-and-problem-statement","text":"We understand the value of automated code qualitative analysis and that it's often an afterthought. Should we bundle a static code analysis component?","title":"Context and Problem Statement"},{"location":"adr/0015-sonarqube-for-application-static-code-analysis/#decision-outcome","text":"We'll use Sonarqube for application static code analysis. As part of our effort to offer a ready best-practices platform we'll setup Sonarqube and tie it into template or default CI/CD pipelines.","title":"Decision Outcome"},{"location":"adr/0015-sonarqube-for-application-static-code-analysis/#links","text":"Sonarqube","title":"Links "},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/","text":"Elastic Cloud for Observability Data Aggregation and Visualization \u00b6 Status: accepted Deciders: @jam01, @k2merlinsix Date: 2020-10 Context and Problem Statement \u00b6 In a microservices architectural system observability is imperative, what components should we use for that? Decision Drivers \u00b6 Cost Feature set Considered Options \u00b6 Elastic Cloud Jaeger Decision Outcome \u00b6 We'll use Elastic Cloud on Kubernetes for Observability Data Aggregation and Visualization. Elastic, Logstash and Kibana offer what we need for logging data. Aditionally, we can use APM server to get trace data from either Elastic's own apm agent or a Jaeger client. Positive Consequences \u00b6 A single pane of glass for observability Less components to coordinate Negative Consequences \u00b6 Valuable service dependency DAG only on license subscription Links \u00b6 Elastic Cloud on Kubernetes Elastic Stack Pricing Elastic with Jaeger ECK project Follow up decision to use Jaeger for Tracing ADR-0017","title":"Elastic Cloud for Observability Data Aggregation and Visualization"},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#elastic-cloud-for-observability-data-aggregation-and-visualization","text":"Status: accepted Deciders: @jam01, @k2merlinsix Date: 2020-10","title":"Elastic Cloud for Observability Data Aggregation and Visualization"},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#context-and-problem-statement","text":"In a microservices architectural system observability is imperative, what components should we use for that?","title":"Context and Problem Statement"},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#decision-drivers","text":"Cost Feature set","title":"Decision Drivers "},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#considered-options","text":"Elastic Cloud Jaeger","title":"Considered Options"},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#decision-outcome","text":"We'll use Elastic Cloud on Kubernetes for Observability Data Aggregation and Visualization. Elastic, Logstash and Kibana offer what we need for logging data. Aditionally, we can use APM server to get trace data from either Elastic's own apm agent or a Jaeger client.","title":"Decision Outcome"},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#positive-consequences","text":"A single pane of glass for observability Less components to coordinate","title":"Positive Consequences "},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#negative-consequences","text":"Valuable service dependency DAG only on license subscription","title":"Negative Consequences "},{"location":"adr/0016-elastic-cloud-for-observability-data-aggregation-and-visualization/#links","text":"Elastic Cloud on Kubernetes Elastic Stack Pricing Elastic with Jaeger ECK project Follow up decision to use Jaeger for Tracing ADR-0017","title":"Links "},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/","text":"Jaeger for Tracing with Elasticsearch Backend \u00b6 Status: accepted Deciders: @jam01 Date: 2020-10 Context and Problem Statement \u00b6 In a microservices architectural system observability is imperative, and Elastic Stack Open Source does not offer service dependency DAG. Can/should we fill in that feature with another component? Decision Drivers \u00b6 Cost Feature set Considered Options \u00b6 Elastic Stack Jaeger Decision Outcome \u00b6 We'll use Jaeger by default for tracing data, with Elasticsearch as the backend. Using Jaeger enables a service DAG that's very valuable and using elasticsearch as the backend enables data aggregation and analysis in Kibana. Positive Consequences \u00b6 Value of the service DAG dependency graph Maintain the ability to cross reference logs and trace data in a single pane Jaeger has closer participation to OpenTracing/Telemetry projects than Elastic Negative Consequences \u00b6 Another integration point Must correlate logs and traces in Camel through MDC Links \u00b6 Jaeger Elastic with Jaeger Jaeger Operator Article Jaeger Elasticsearch and Kibana Article Distributed Tracing with Jaeger and the ELK Stack Article Exploring Jaeger traces with Elastic APM","title":"Jaeger for Tracing with Elasticsearch Backend"},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#jaeger-for-tracing-with-elasticsearch-backend","text":"Status: accepted Deciders: @jam01 Date: 2020-10","title":"Jaeger for Tracing with Elasticsearch Backend"},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#context-and-problem-statement","text":"In a microservices architectural system observability is imperative, and Elastic Stack Open Source does not offer service dependency DAG. Can/should we fill in that feature with another component?","title":"Context and Problem Statement"},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#decision-drivers","text":"Cost Feature set","title":"Decision Drivers "},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#considered-options","text":"Elastic Stack Jaeger","title":"Considered Options"},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#decision-outcome","text":"We'll use Jaeger by default for tracing data, with Elasticsearch as the backend. Using Jaeger enables a service DAG that's very valuable and using elasticsearch as the backend enables data aggregation and analysis in Kibana.","title":"Decision Outcome"},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#positive-consequences","text":"Value of the service DAG dependency graph Maintain the ability to cross reference logs and trace data in a single pane Jaeger has closer participation to OpenTracing/Telemetry projects than Elastic","title":"Positive Consequences "},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#negative-consequences","text":"Another integration point Must correlate logs and traces in Camel through MDC","title":"Negative Consequences "},{"location":"adr/0017-jaeger-for-tracing-with-elasticsearch-backend/#links","text":"Jaeger Elastic with Jaeger Jaeger Operator Article Jaeger Elasticsearch and Kibana Article Distributed Tracing with Jaeger and the ELK Stack Article Exploring Jaeger traces with Elastic APM","title":"Links "},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/","text":"Nexus Repository Manager for Artifact Management \u00b6 Status: accepted Deciders: @jam01, @k2merlinsix Date: 2020-10 Context and Problem Statement \u00b6 In a solid continuous delivery configuration an enterprise needs to have a history of application builds an ability to rollback deployments as necessary. What artifact manager component should we use? Decision Drivers \u00b6 Stability Considered Options \u00b6 Nexus Repository Manager JFrog Artifactory Decision Outcome \u00b6 We'll use Nexus repository manager for artifact management. Sonatype is one of the main stewards/contributors of Maven and therefore their platform is one of the most mature. In order to deliver a best practices continuous delivery configuration we'll tie in Nexus into our CI/CD pipelines. Positive Consequences \u00b6 Available commercial support from Nexus Links \u00b6 Nexus Repository Manager OSS JFrog Artifactory","title":"Nexus Repository Manager for Artifact Management"},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#nexus-repository-manager-for-artifact-management","text":"Status: accepted Deciders: @jam01, @k2merlinsix Date: 2020-10","title":"Nexus Repository Manager for Artifact Management"},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#context-and-problem-statement","text":"In a solid continuous delivery configuration an enterprise needs to have a history of application builds an ability to rollback deployments as necessary. What artifact manager component should we use?","title":"Context and Problem Statement"},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#decision-drivers","text":"Stability","title":"Decision Drivers "},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#considered-options","text":"Nexus Repository Manager JFrog Artifactory","title":"Considered Options"},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#decision-outcome","text":"We'll use Nexus repository manager for artifact management. Sonatype is one of the main stewards/contributors of Maven and therefore their platform is one of the most mature. In order to deliver a best practices continuous delivery configuration we'll tie in Nexus into our CI/CD pipelines.","title":"Decision Outcome"},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#positive-consequences","text":"Available commercial support from Nexus","title":"Positive Consequences "},{"location":"adr/0018-nexus-repository-manager-for-artifact-management/#links","text":"Nexus Repository Manager OSS JFrog Artifactory","title":"Links "},{"location":"adr/0019-prefer-daemonsets-over-sidecars/","text":"Prefer Daemonsets over Sidecars \u00b6 Status: accepted Deciders: @jam01 Date: 2020-11 Context and Problem Statement \u00b6 Kubernetes supports deploying workloads as daemonsets or sidecars. For example, when deploying Jaeger Agent for reporting trace data, in a DaemonSet strategy there will be a single Agent deployment in every Kubernetes node, so all applications' Jaeger Client in a single node share the same agent. Conversely, in a side-car strategy each application will have its own dedicated Jaeger Agent deployment, requiring extra resources. Which should we favor for cross cutting concern deployments? Decision Drivers \u00b6 Cost Considered Options \u00b6 Sidecars Daemonsets Decision Outcome \u00b6 We'll prefer daemonsets over sidecar deployments whenever there is the option. Given that Tavros is designed as a single tenant platform there are computing resource savings when using daemonsets (deployment per node) vs sidecars (deployment per service). Positive Consequences \u00b6 Less resource utilization Negative Consequences \u00b6 Would have to refactor in order to support multi-tenancy Links \u00b6 DaemonSet See about single tenancy in ADR-0029","title":"Prefer Daemonsets over Sidecars"},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#prefer-daemonsets-over-sidecars","text":"Status: accepted Deciders: @jam01 Date: 2020-11","title":"Prefer Daemonsets over Sidecars"},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#context-and-problem-statement","text":"Kubernetes supports deploying workloads as daemonsets or sidecars. For example, when deploying Jaeger Agent for reporting trace data, in a DaemonSet strategy there will be a single Agent deployment in every Kubernetes node, so all applications' Jaeger Client in a single node share the same agent. Conversely, in a side-car strategy each application will have its own dedicated Jaeger Agent deployment, requiring extra resources. Which should we favor for cross cutting concern deployments?","title":"Context and Problem Statement"},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#decision-drivers","text":"Cost","title":"Decision Drivers "},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#considered-options","text":"Sidecars Daemonsets","title":"Considered Options"},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#decision-outcome","text":"We'll prefer daemonsets over sidecar deployments whenever there is the option. Given that Tavros is designed as a single tenant platform there are computing resource savings when using daemonsets (deployment per node) vs sidecars (deployment per service).","title":"Decision Outcome"},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#positive-consequences","text":"Less resource utilization","title":"Positive Consequences "},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#negative-consequences","text":"Would have to refactor in order to support multi-tenancy","title":"Negative Consequences "},{"location":"adr/0019-prefer-daemonsets-over-sidecars/#links","text":"DaemonSet See about single tenancy in ADR-0029","title":"Links "},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/","text":"Spring Cloud Config for Application Configuration Management \u00b6 Status: accepted Deciders: @jam01 Date: 2020-11 Context and Problem Statement \u00b6 In a solid continuous delivery configuration an enterprise needs to manage application configuration properties independently of the application source code. What component should we use? Decision Drivers \u00b6 Flexible backend support Easy integration with Camel and Spring Boot Considered Options \u00b6 Spring Cloud Config Decision Outcome \u00b6 We'll use spring cloud config as our application configuration manager. Spring cloud config is very simple to integrate since we chose Spring Boot as our base application framework. We'll use a Git backend from Gitea which will allow our customers to keep a history of changes and enable rollback. Positive Consequences \u00b6 Simple integration to Spring Boot Links \u00b6 Spring Cloud Config","title":"Spring Cloud Config for Application Configuration Management"},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#spring-cloud-config-for-application-configuration-management","text":"Status: accepted Deciders: @jam01 Date: 2020-11","title":"Spring Cloud Config for Application Configuration Management"},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#context-and-problem-statement","text":"In a solid continuous delivery configuration an enterprise needs to manage application configuration properties independently of the application source code. What component should we use?","title":"Context and Problem Statement"},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#decision-drivers","text":"Flexible backend support Easy integration with Camel and Spring Boot","title":"Decision Drivers "},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#considered-options","text":"Spring Cloud Config","title":"Considered Options"},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#decision-outcome","text":"We'll use spring cloud config as our application configuration manager. Spring cloud config is very simple to integrate since we chose Spring Boot as our base application framework. We'll use a Git backend from Gitea which will allow our customers to keep a history of changes and enable rollback.","title":"Decision Outcome"},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#positive-consequences","text":"Simple integration to Spring Boot","title":"Positive Consequences "},{"location":"adr/0020-spring-cloud-config-for-application-configuration-management/#links","text":"Spring Cloud Config","title":"Links "},{"location":"adr/0021-prefer-kong-enterprise-edition/","text":"Prefer Kong Enterprise Edition \u00b6 Status: accepted Deciders: @k2merlinsix Date: 2020-11 Context and Problem Statement \u00b6 Given that we've chosen Kong as our Ingress Controller and API Gateway, but we're still missing an API Manager and Developer Portal. Decision Drivers \u00b6 Feature set Decision Outcome \u00b6 We'll recommend Kong enterprise edition in order to enable API manager and developer portal features. API manager and developer portal features are not necessary but they are valuable, supporting Kong EE as an optional component is an acceptable compromise to offer that value to our customers. Positive Consequences \u00b6 Single component for all API related functionality Negative Consequences \u00b6 Requires a license Links \u00b6 Kong Enterprise","title":"Prefer Kong Enterprise Edition"},{"location":"adr/0021-prefer-kong-enterprise-edition/#prefer-kong-enterprise-edition","text":"Status: accepted Deciders: @k2merlinsix Date: 2020-11","title":"Prefer Kong Enterprise Edition"},{"location":"adr/0021-prefer-kong-enterprise-edition/#context-and-problem-statement","text":"Given that we've chosen Kong as our Ingress Controller and API Gateway, but we're still missing an API Manager and Developer Portal.","title":"Context and Problem Statement"},{"location":"adr/0021-prefer-kong-enterprise-edition/#decision-drivers","text":"Feature set","title":"Decision Drivers "},{"location":"adr/0021-prefer-kong-enterprise-edition/#decision-outcome","text":"We'll recommend Kong enterprise edition in order to enable API manager and developer portal features. API manager and developer portal features are not necessary but they are valuable, supporting Kong EE as an optional component is an acceptable compromise to offer that value to our customers.","title":"Decision Outcome"},{"location":"adr/0021-prefer-kong-enterprise-edition/#positive-consequences","text":"Single component for all API related functionality","title":"Positive Consequences "},{"location":"adr/0021-prefer-kong-enterprise-edition/#negative-consequences","text":"Requires a license","title":"Negative Consequences "},{"location":"adr/0021-prefer-kong-enterprise-edition/#links","text":"Kong Enterprise","title":"Links "},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/","text":"Use Ansible as Provisioning Engine \u00b6 Status: accepted Deciders: Mohammad Naeem, @jam01 Date: 2020-11 Context and Problem Statement \u00b6 [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] Decision Drivers \u00b6 Flexibility Maintainability Considered Options \u00b6 Bash Ansible Decision Outcome \u00b6 We'll use Ansible as the provisioning engine. Ansible modules, variable handling, Jinja 2 templating, and Kubernetes module make it easier to install and configure components. Helm Charts use the same templating language, and the Operator SDK has support for Ansible, if we decide to build an Operator. Positive Consequences \u00b6 Ansible being a desired-state engine enables idempotency Simpler setup Negative Consequences \u00b6 A learning curve to Ansible, playbooks, roles, etc Links \u00b6 Ansible Kubernetes Ansible Ansible Operator SDK","title":"Use Ansible as Provisioning Engine"},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#use-ansible-as-provisioning-engine","text":"Status: accepted Deciders: Mohammad Naeem, @jam01 Date: 2020-11","title":"Use Ansible as Provisioning Engine"},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#context-and-problem-statement","text":"[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]","title":"Context and Problem Statement"},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#decision-drivers","text":"Flexibility Maintainability","title":"Decision Drivers "},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#considered-options","text":"Bash Ansible","title":"Considered Options"},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#decision-outcome","text":"We'll use Ansible as the provisioning engine. Ansible modules, variable handling, Jinja 2 templating, and Kubernetes module make it easier to install and configure components. Helm Charts use the same templating language, and the Operator SDK has support for Ansible, if we decide to build an Operator.","title":"Decision Outcome"},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#positive-consequences","text":"Ansible being a desired-state engine enables idempotency Simpler setup","title":"Positive Consequences "},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#negative-consequences","text":"A learning curve to Ansible, playbooks, roles, etc","title":"Negative Consequences "},{"location":"adr/0022-use-ansible-as-the-provisioning-engine/#links","text":"Ansible Kubernetes Ansible Ansible Operator SDK","title":"Links "},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/","text":"Helm and Operators for Component Installation and Management \u00b6 Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11 Context and Problem Statement \u00b6 We're currently installing components ad-hoc through directly modified Kubernetes manifests from helm charts. This removes the ability to use Helm's upgrade features. This is done because we can't use Helm until Flux is up and running, and Flux depends on other components being installed before. Decision Drivers \u00b6 Ability to use Helm upgrades Minimize hand crafted manifests Considered Options \u00b6 Helm releases through Flux v2 Operators Decision Outcome \u00b6 We'll use Helm releases and Operators custom resources for component installation and management. Given that we're now using Flux v2 we can use their Helm release functionality before any Git repository is available, this way we can do Helm releases through Flux and eventually commit them to Git to enable GitOps from that point on. Kubernets Operators enable a higher level of component lifecycle management, these should always be preferred to Helm whenever available. Positive Consequences \u00b6 Upon provisioning the platform it will be entirely driven by GitOps Will be able to do component upgrades through Helm releases Less custom code by using official functionality as available Links \u00b6 Helm Operator Pattern Operator Hub Flux v2 Helm Controller","title":"Helm and Operators for Component Installation and Management"},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#helm-and-operators-for-component-installation-and-management","text":"Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11","title":"Helm and Operators for Component Installation and Management"},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#context-and-problem-statement","text":"We're currently installing components ad-hoc through directly modified Kubernetes manifests from helm charts. This removes the ability to use Helm's upgrade features. This is done because we can't use Helm until Flux is up and running, and Flux depends on other components being installed before.","title":"Context and Problem Statement"},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#decision-drivers","text":"Ability to use Helm upgrades Minimize hand crafted manifests","title":"Decision Drivers "},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#considered-options","text":"Helm releases through Flux v2 Operators","title":"Considered Options"},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#decision-outcome","text":"We'll use Helm releases and Operators custom resources for component installation and management. Given that we're now using Flux v2 we can use their Helm release functionality before any Git repository is available, this way we can do Helm releases through Flux and eventually commit them to Git to enable GitOps from that point on. Kubernets Operators enable a higher level of component lifecycle management, these should always be preferred to Helm whenever available.","title":"Decision Outcome"},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#positive-consequences","text":"Upon provisioning the platform it will be entirely driven by GitOps Will be able to do component upgrades through Helm releases Less custom code by using official functionality as available","title":"Positive Consequences "},{"location":"adr/0023-helm-and-operators-for-component-installation-and-management/#links","text":"Helm Operator Pattern Operator Hub Flux v2 Helm Controller","title":"Links "},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/","text":"Use Ansible Collection to Structure and Package Ansible Code \u00b6 Status: accepted Deciders: @jam01 Date: 2020-11 Context and Problem Statement \u00b6 How do we package multiple playbooks, plugins, and how do we distribute them when creating a customer's platform? Designing pre-configured components and their extension points through Kustomizations is increasingly complex. Is there a way to simplify and make it more maintainable? Decision Drivers \u00b6 Maintainability of the code Easier to add customer configurability of components Support for distribution Considered Options \u00b6 Ansible Collection Decision Outcome \u00b6 We'll use Ansible Collection structure and packaging for our Ansible code. We'll refactor the platform from multiple layers of Kustomizations to be entirely driven by an Ansible Collection. Ansible is migrating all their re-usable artifacts to be Collections so that seems to be where all support and tooling is to be found, including distribution from a git repository. Moving the component configuration and extension points to Ansible makes it much easier to structure and maintain, as well as add points of extension. Positive Consequences \u00b6 Higher maintainability as it's all Ansible Easier to add customer configurability as everything can be templated Negative Consequences \u00b6 A bit more complexity Links \u00b6 Using Collections Blog The Future of Ansible Content Delivery Blog Hands on with Ansible collections","title":"Use Ansible Collection to Structure and Package Ansible Code"},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#use-ansible-collection-to-structure-and-package-ansible-code","text":"Status: accepted Deciders: @jam01 Date: 2020-11","title":"Use Ansible Collection to Structure and Package Ansible Code"},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#context-and-problem-statement","text":"How do we package multiple playbooks, plugins, and how do we distribute them when creating a customer's platform? Designing pre-configured components and their extension points through Kustomizations is increasingly complex. Is there a way to simplify and make it more maintainable?","title":"Context and Problem Statement"},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#decision-drivers","text":"Maintainability of the code Easier to add customer configurability of components Support for distribution","title":"Decision Drivers "},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#considered-options","text":"Ansible Collection","title":"Considered Options"},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#decision-outcome","text":"We'll use Ansible Collection structure and packaging for our Ansible code. We'll refactor the platform from multiple layers of Kustomizations to be entirely driven by an Ansible Collection. Ansible is migrating all their re-usable artifacts to be Collections so that seems to be where all support and tooling is to be found, including distribution from a git repository. Moving the component configuration and extension points to Ansible makes it much easier to structure and maintain, as well as add points of extension.","title":"Decision Outcome"},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#positive-consequences","text":"Higher maintainability as it's all Ansible Easier to add customer configurability as everything can be templated","title":"Positive Consequences "},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#negative-consequences","text":"A bit more complexity","title":"Negative Consequences "},{"location":"adr/0024-use-ansible-collection-to-structure-and-package-ansible-code/#links","text":"Using Collections Blog The Future of Ansible Content Delivery Blog Hands on with Ansible collections","title":"Links "},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/","text":"Setup Sandbox and Production Kuma Meshes and Kong Ingress Controllers \u00b6 Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11 Context and Problem Statement \u00b6 What should be our default service mesh and application environments look like? Given that we'll enable mutual TLS on the meshes, each mesh will require a Kong Ingress Controller which in turn requires a Load Balancer from the cloud provider. Decision Drivers \u00b6 Must reflect best practice configuration Segregate production from non-production workloads as much as possible Satisfy 'common' enterprise requirements Cost Considered Options \u00b6 One dev, one test, and one production environment, all segregated through Kuma meshes Dev and test environments separated by kuma meshes in one cluster, production in another Dev and test environments in one 'sandbox' kuma mesh, production in a 'production' mesh Decision Outcome \u00b6 We'll setup a Sandbox and Production Kuma mesh, sandbox will have a dev and test namespaced environment, and production will have the production namespaced environment. The service mesh and mutual TLS plugin enforces the segregation of non-production (aka sandbox) environments and the production environment to where we don't fuctionally need a separate cluster. Moreover aggregating dev and test into a sandbox mesh saves us provisioning a load balancer for each environment, so we could support any arbitrary environment configuration needs. Positive Consequences \u00b6 Reduced cost of extra load balancer Reduced complexity of dedicated cluster Negative Consequences \u00b6 Learning curve of service mesh mutual TLS enforcement Sandboxed environments can still communicate with each other Links \u00b6 Kuma mTLS Kuma Gateway","title":"Setup Sandbox and Production Kuma Meshes and Kong Ingress Controllers"},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers","text":"Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11","title":"Setup Sandbox and Production Kuma Meshes and Kong Ingress Controllers"},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#context-and-problem-statement","text":"What should be our default service mesh and application environments look like? Given that we'll enable mutual TLS on the meshes, each mesh will require a Kong Ingress Controller which in turn requires a Load Balancer from the cloud provider.","title":"Context and Problem Statement"},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#decision-drivers","text":"Must reflect best practice configuration Segregate production from non-production workloads as much as possible Satisfy 'common' enterprise requirements Cost","title":"Decision Drivers "},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#considered-options","text":"One dev, one test, and one production environment, all segregated through Kuma meshes Dev and test environments separated by kuma meshes in one cluster, production in another Dev and test environments in one 'sandbox' kuma mesh, production in a 'production' mesh","title":"Considered Options"},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#decision-outcome","text":"We'll setup a Sandbox and Production Kuma mesh, sandbox will have a dev and test namespaced environment, and production will have the production namespaced environment. The service mesh and mutual TLS plugin enforces the segregation of non-production (aka sandbox) environments and the production environment to where we don't fuctionally need a separate cluster. Moreover aggregating dev and test into a sandbox mesh saves us provisioning a load balancer for each environment, so we could support any arbitrary environment configuration needs.","title":"Decision Outcome"},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#positive-consequences","text":"Reduced cost of extra load balancer Reduced complexity of dedicated cluster","title":"Positive Consequences "},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#negative-consequences","text":"Learning curve of service mesh mutual TLS enforcement Sandboxed environments can still communicate with each other","title":"Negative Consequences "},{"location":"adr/0025-setup-sandbox-and-production-kuma-meshes-and-kong-ingress-controllers/#links","text":"Kuma mTLS Kuma Gateway","title":"Links "},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/","text":"Setup Sandbox and Production Keycloak Realms \u00b6 Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11 Context and Problem Statement \u00b6 Keycloak enables separation of sets of users, groups, roles, etc. through Realms. Should we create separate realms by default, which ones? Decision Drivers \u00b6 Must reflect best practice configuration Segregate production from non-production workloads as much as possible Satisfy 'common' enterprise requirements Considered Options \u00b6 Leave single Master realm Create one production realm Create sandbox and production realms Decision Outcome \u00b6 We'll create sandbox and production keycloak realms. Given the existing inclusion of sandbox and production concepts through service mesh and ingress controllers, these realms make a simple and easy to understand separation of users that allows customers to also test and promote identity and access management policies in a similar way to application workloads. We've seen enterprises do this with okta and okta preview instances for example. Positive Consequences \u00b6 Simple, easy to understand Enables identity and access management separation Negative Consequences \u00b6 May be unnecessary for some organizations Links \u00b6 Keycloak Concetps","title":"Setup Sandbox and Production Keycloak Realms"},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#setup-sandbox-and-production-keycloak-realms","text":"Status: accepted Deciders: @jam01, @rmccright-ms3 Date: 2020-11","title":"Setup Sandbox and Production Keycloak Realms"},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#context-and-problem-statement","text":"Keycloak enables separation of sets of users, groups, roles, etc. through Realms. Should we create separate realms by default, which ones?","title":"Context and Problem Statement"},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#decision-drivers","text":"Must reflect best practice configuration Segregate production from non-production workloads as much as possible Satisfy 'common' enterprise requirements","title":"Decision Drivers "},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#considered-options","text":"Leave single Master realm Create one production realm Create sandbox and production realms","title":"Considered Options"},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#decision-outcome","text":"We'll create sandbox and production keycloak realms. Given the existing inclusion of sandbox and production concepts through service mesh and ingress controllers, these realms make a simple and easy to understand separation of users that allows customers to also test and promote identity and access management policies in a similar way to application workloads. We've seen enterprises do this with okta and okta preview instances for example.","title":"Decision Outcome"},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#positive-consequences","text":"Simple, easy to understand Enables identity and access management separation","title":"Positive Consequences "},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#negative-consequences","text":"May be unnecessary for some organizations","title":"Negative Consequences "},{"location":"adr/0026-setup-sandbox-and-production-keycloak-realms/#links","text":"Keycloak Concetps","title":"Links "},{"location":"adr/0027-cert-manager-for-certificate-management/","text":"cert-manager for Certificate Management \u00b6 Status: accepted Deciders: @jam01 Date: 2020-11 Context and Problem Statement \u00b6 The platform will required TLS certificates to signed by well known CAs. What tool do we use for generate those certificates? Decision Drivers \u00b6 Automation, including rotation before expiration Cost Considered Options \u00b6 cert-manager Decision Outcome \u00b6 We'll use cert-manager as our certificate manager. cert-manager is the best known certificate manager for Kubernetes, it will automatically generate certificates as they're request by Ingress TLS configuration and will automatically re-generate them before they expire. For CA we'll use Let's Encrypt which is free. Positive Consequences \u00b6 Automatic generation and re-generation Negative Consequences \u00b6 Let's Encrypt limit of 50 certificates per week Links \u00b6 cert-manager Securing Ingress Resources ACME Issuers Let's Encrypt Docs","title":"cert-manager for Certificate Management"},{"location":"adr/0027-cert-manager-for-certificate-management/#cert-manager-for-certificate-management","text":"Status: accepted Deciders: @jam01 Date: 2020-11","title":"cert-manager for Certificate Management"},{"location":"adr/0027-cert-manager-for-certificate-management/#context-and-problem-statement","text":"The platform will required TLS certificates to signed by well known CAs. What tool do we use for generate those certificates?","title":"Context and Problem Statement"},{"location":"adr/0027-cert-manager-for-certificate-management/#decision-drivers","text":"Automation, including rotation before expiration Cost","title":"Decision Drivers "},{"location":"adr/0027-cert-manager-for-certificate-management/#considered-options","text":"cert-manager","title":"Considered Options"},{"location":"adr/0027-cert-manager-for-certificate-management/#decision-outcome","text":"We'll use cert-manager as our certificate manager. cert-manager is the best known certificate manager for Kubernetes, it will automatically generate certificates as they're request by Ingress TLS configuration and will automatically re-generate them before they expire. For CA we'll use Let's Encrypt which is free.","title":"Decision Outcome"},{"location":"adr/0027-cert-manager-for-certificate-management/#positive-consequences","text":"Automatic generation and re-generation","title":"Positive Consequences "},{"location":"adr/0027-cert-manager-for-certificate-management/#negative-consequences","text":"Let's Encrypt limit of 50 certificates per week","title":"Negative Consequences "},{"location":"adr/0027-cert-manager-for-certificate-management/#links","text":"cert-manager Securing Ingress Resources ACME Issuers Let's Encrypt Docs","title":"Links "},{"location":"adr/0028-use-markdown-architectural-decision-records/","text":"Use Markdown Architectural Decision Records \u00b6 Context and Problem Statement \u00b6 We want to record architectural decisions made in this project. Which format and structure should these records follow? Considered Options \u00b6 MADR 2.1.2 \u2013 The Markdown Architectural Decision Records Michael Nygard's template \u2013 The first incarnation of the term \"ADR\" Sustainable Architectural Decisions \u2013 The Y-Statements Other templates listed at https://github.com/joelparkerhenderson/architecture_decision_record Formless \u2013 No conventions for file format and structure Decision Outcome \u00b6 Chosen option: \"MADR 2.1.2\", because Implicit assumptions should be made explicit. Design documentation is important to enable people understanding the decisions later on. See also A rational design process: How and why to fake it . The MADR format is lean and fits our development style. The MADR structure is comprehensible and facilitates usage & maintenance. The MADR project is vivid. Version 2.1.2 is the latest one available when starting to document ADRs.","title":"Use Markdown Architectural Decision Records"},{"location":"adr/0028-use-markdown-architectural-decision-records/#use-markdown-architectural-decision-records","text":"","title":"Use Markdown Architectural Decision Records"},{"location":"adr/0028-use-markdown-architectural-decision-records/#context-and-problem-statement","text":"We want to record architectural decisions made in this project. Which format and structure should these records follow?","title":"Context and Problem Statement"},{"location":"adr/0028-use-markdown-architectural-decision-records/#considered-options","text":"MADR 2.1.2 \u2013 The Markdown Architectural Decision Records Michael Nygard's template \u2013 The first incarnation of the term \"ADR\" Sustainable Architectural Decisions \u2013 The Y-Statements Other templates listed at https://github.com/joelparkerhenderson/architecture_decision_record Formless \u2013 No conventions for file format and structure","title":"Considered Options"},{"location":"adr/0028-use-markdown-architectural-decision-records/#decision-outcome","text":"Chosen option: \"MADR 2.1.2\", because Implicit assumptions should be made explicit. Design documentation is important to enable people understanding the decisions later on. See also A rational design process: How and why to fake it . The MADR format is lean and fits our development style. The MADR structure is comprehensible and facilitates usage & maintenance. The MADR project is vivid. Version 2.1.2 is the latest one available when starting to document ADRs.","title":"Decision Outcome"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/","text":"Tavros as a Single Tenant Platform \u00b6 Status: accepted Deciders: @jam01 Date: 2020-11 Context and Problem Statement \u00b6 A single platform for an indefinite amount of tenants would allow us to provide Tavros in a cloud 'as-a-service' offering to our customers, instead of only as managed services. However, there is an unknown complexity and level of effort behind multi-tenancy. Decision Drivers \u00b6 Scope creep Speed of delivery Complexity Considered Options \u00b6 Build multi-tenancy from the ground up Automated single-tenant offered as PaaS Decision Outcome \u00b6 We'll build Tavros as a single tenant platform. The Ansible automation can be developed in a way that accommodates two modes of operation: Build a tavros cluster to be owned by the client and optionally operated by us, as was the original objective Build a tavros cluster to be owned and operated by us, transparently to the client. The second mode allows us to offer a PaaS while keeping the platform simple. Positive Consequences \u00b6 Explicit separation to other clusters Simpler to build and consequently faster to deliver Can still offer as PaaS Negative Consequences \u00b6 May complicate day-2 operations on multiple clusters at once","title":"Tavros as a Single Tenant Platform"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#tavros-as-a-single-tenant-platform","text":"Status: accepted Deciders: @jam01 Date: 2020-11","title":"Tavros as a Single Tenant Platform"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#context-and-problem-statement","text":"A single platform for an indefinite amount of tenants would allow us to provide Tavros in a cloud 'as-a-service' offering to our customers, instead of only as managed services. However, there is an unknown complexity and level of effort behind multi-tenancy.","title":"Context and Problem Statement"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#decision-drivers","text":"Scope creep Speed of delivery Complexity","title":"Decision Drivers "},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#considered-options","text":"Build multi-tenancy from the ground up Automated single-tenant offered as PaaS","title":"Considered Options"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#decision-outcome","text":"We'll build Tavros as a single tenant platform. The Ansible automation can be developed in a way that accommodates two modes of operation: Build a tavros cluster to be owned by the client and optionally operated by us, as was the original objective Build a tavros cluster to be owned and operated by us, transparently to the client. The second mode allows us to offer a PaaS while keeping the platform simple.","title":"Decision Outcome"},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#positive-consequences","text":"Explicit separation to other clusters Simpler to build and consequently faster to deliver Can still offer as PaaS","title":"Positive Consequences "},{"location":"adr/0029-troubadour-as-a-single-tenant-platform/#negative-consequences","text":"May complicate day-2 operations on multiple clusters at once","title":"Negative Consequences "},{"location":"adr/template/","text":"[short title of solved problem and solution] \u00b6 Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 ] Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL] Context and Problem Statement \u00b6 [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] Decision Drivers \u00b6 [driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026 Considered Options \u00b6 [option 1] [option 2] [option 3] \u2026 Decision Outcome \u00b6 Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. Positive Consequences \u00b6 [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026 Negative Consequences \u00b6 [e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026 Pros and Cons of the Options \u00b6 [option 1] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 2] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 3] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 Links \u00b6 [Link type] [Link to ADR] \u2026","title":"[short title of solved problem and solution]"},{"location":"adr/template/#short-title-of-solved-problem-and-solution","text":"Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 ] Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL]","title":"[short title of solved problem and solution]"},{"location":"adr/template/#context-and-problem-statement","text":"[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]","title":"Context and Problem Statement"},{"location":"adr/template/#decision-drivers","text":"[driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026","title":"Decision Drivers "},{"location":"adr/template/#considered-options","text":"[option 1] [option 2] [option 3] \u2026","title":"Considered Options"},{"location":"adr/template/#decision-outcome","text":"Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].","title":"Decision Outcome"},{"location":"adr/template/#positive-consequences","text":"[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026","title":"Positive Consequences "},{"location":"adr/template/#negative-consequences","text":"[e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026","title":"Negative Consequences "},{"location":"adr/template/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options "},{"location":"adr/template/#option-1","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 1]"},{"location":"adr/template/#option-2","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 2]"},{"location":"adr/template/#option-3","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 3]"},{"location":"adr/template/#links","text":"[Link type] [Link to ADR] \u2026","title":"Links "},{"location":"getting-started/installing/","text":"Getting Started with Tavros \u00b6 Prerequisites \u00b6 Install Container Runtime \u00b6 We provide a Tavros Collection Image pre-installed with the latest version of Tavros and all its required Tooling. To utilize this you will need a Container Runtime such as Docker or Podman. Get Docker Get Podman Prepare Kubernetes Cluster \u00b6 Tavros is Kubernetes Native so you will need access to a Cluster to provision its components. You can utilize an existing Cluster or have Tavros provision a new one for you. Existing New AKS New AWS For Existing Clusters you will need to provide Client access to the Tavros container via a Kube config file. Your cluster must meet the following requirements: K8s 1.19,1.20,1.21 Has a default Storage Class Supports Service of Type Loadbalancer Azure Kubernetes Services (AKS) Login to Azure az login Create Service Principal az ad sp create-for-rbac --name tavros --role owner --sdk-auth Create Resource group az group create -n tavros-aks --location eastus Create Storage Account az storage account create -n tavros01234 -g tavros-aks Create DNS Zone az network dns zone create -n az.ms3-inc.com -g tavros-aks AWS w/ Kops #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } Select Playbook \u00b6 Currently we have a single Default Provisioning Playbook. Prepare Inventory File \u00b6 --- all: vars: cluster_fqdn: '' cluster_admin_email: '' kubernetes_cluster: cloud_provider: 'aws' dns_zone: \"{{ cluster_fqdn }}\" keycloak: realm: prod kops: enabled: true master_count: 3 master_size: 't2.large' master_zones: 'us-east-1a,us-east-1b' node_count: 2 node_size: 't2.xlarge' node_zones: 'us-east-1a,us-east-1b' state_bucket: \"{{ cluster_fqdn }}-tavros\" state_bucket_region: \"us-east-1\" kong: enabled: true default_ingress_class: 'prod' ee_creds: [] instances: - name: 'sandbox-kong' hybrid: false ingress_class: 'sandbox' kuma_mesh_name: 'sandbox' ee: enabled: false - name: 'prod-kong' hybrid: true ingress_class: 'prod' kuma_mesh_name: 'prod' ee: enabled: false kuma: enabled: true meshes: - name: 'sandbox' mtls: enabled: true traffictrace: enabled: true - name: 'prod' mtls: enabled: true traffictrace: enabled: true namespaces: - name: 'dev' kuma_mesh_name: 'sandbox' - name: 'test' kuma_mesh_name: 'sandbox' - name: 'prod' kuma_mesh_name: 'prod' keycloak: enabled: true realms: - name: sandbox - name: prod postgresql: {} cert_manager: enabled: true gitea: enabled: true keycloak: realm: 'prod' nexus: enabled: true keycloak: realm: 'prod' elastic_cloud: ee: enabled: false enabled: true jaeger: enabled: true keycloak: realm: 'prod' jenkins: enabled: true keycloak: realm: 'prod' flux: {} Installing \u00b6 Pull Tavros Collection \u00b6 docker pull ghcr.io/ms3inc/tavros-collection Shell into Tavros Collection Instance \u00b6 docker run -it --rm -v $PWD:$PWD:Z -w $PWD ghcr.io/ms3inc/tavros-collection:latest Run Tavros Provision Playbook \u00b6 You can dry run first to make sure there are no issues with the Templeting phase ansible-playbook playbooks/provision_playbook.yaml --inventory <inventory_path>.yaml --tags all,dry-run Provisoion the Tavros Cluster with ansible-playbook playbooks/provision_playbook.yaml --inventory <inventory_path>.yaml --tags all","title":"Installing"},{"location":"getting-started/installing/#getting-started-with-tavros","text":"","title":"Getting Started with Tavros"},{"location":"getting-started/installing/#prerequisites","text":"","title":"Prerequisites"},{"location":"getting-started/installing/#install-container-runtime","text":"We provide a Tavros Collection Image pre-installed with the latest version of Tavros and all its required Tooling. To utilize this you will need a Container Runtime such as Docker or Podman. Get Docker Get Podman","title":"Install Container Runtime"},{"location":"getting-started/installing/#prepare-kubernetes-cluster","text":"Tavros is Kubernetes Native so you will need access to a Cluster to provision its components. You can utilize an existing Cluster or have Tavros provision a new one for you. Existing New AKS New AWS For Existing Clusters you will need to provide Client access to the Tavros container via a Kube config file. Your cluster must meet the following requirements: K8s 1.19,1.20,1.21 Has a default Storage Class Supports Service of Type Loadbalancer Azure Kubernetes Services (AKS) Login to Azure az login Create Service Principal az ad sp create-for-rbac --name tavros --role owner --sdk-auth Create Resource group az group create -n tavros-aks --location eastus Create Storage Account az storage account create -n tavros01234 -g tavros-aks Create DNS Zone az network dns zone create -n az.ms3-inc.com -g tavros-aks AWS w/ Kops #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; }","title":"Prepare Kubernetes Cluster"},{"location":"getting-started/installing/#select-playbook","text":"Currently we have a single Default Provisioning Playbook.","title":"Select Playbook"},{"location":"getting-started/installing/#prepare-inventory-file","text":"--- all: vars: cluster_fqdn: '' cluster_admin_email: '' kubernetes_cluster: cloud_provider: 'aws' dns_zone: \"{{ cluster_fqdn }}\" keycloak: realm: prod kops: enabled: true master_count: 3 master_size: 't2.large' master_zones: 'us-east-1a,us-east-1b' node_count: 2 node_size: 't2.xlarge' node_zones: 'us-east-1a,us-east-1b' state_bucket: \"{{ cluster_fqdn }}-tavros\" state_bucket_region: \"us-east-1\" kong: enabled: true default_ingress_class: 'prod' ee_creds: [] instances: - name: 'sandbox-kong' hybrid: false ingress_class: 'sandbox' kuma_mesh_name: 'sandbox' ee: enabled: false - name: 'prod-kong' hybrid: true ingress_class: 'prod' kuma_mesh_name: 'prod' ee: enabled: false kuma: enabled: true meshes: - name: 'sandbox' mtls: enabled: true traffictrace: enabled: true - name: 'prod' mtls: enabled: true traffictrace: enabled: true namespaces: - name: 'dev' kuma_mesh_name: 'sandbox' - name: 'test' kuma_mesh_name: 'sandbox' - name: 'prod' kuma_mesh_name: 'prod' keycloak: enabled: true realms: - name: sandbox - name: prod postgresql: {} cert_manager: enabled: true gitea: enabled: true keycloak: realm: 'prod' nexus: enabled: true keycloak: realm: 'prod' elastic_cloud: ee: enabled: false enabled: true jaeger: enabled: true keycloak: realm: 'prod' jenkins: enabled: true keycloak: realm: 'prod' flux: {}","title":"Prepare Inventory File"},{"location":"getting-started/installing/#installing","text":"","title":"Installing"},{"location":"getting-started/installing/#pull-tavros-collection","text":"docker pull ghcr.io/ms3inc/tavros-collection","title":"Pull Tavros Collection"},{"location":"getting-started/installing/#shell-into-tavros-collection-instance","text":"docker run -it --rm -v $PWD:$PWD:Z -w $PWD ghcr.io/ms3inc/tavros-collection:latest","title":"Shell into Tavros Collection Instance"},{"location":"getting-started/installing/#run-tavros-provision-playbook","text":"You can dry run first to make sure there are no issues with the Templeting phase ansible-playbook playbooks/provision_playbook.yaml --inventory <inventory_path>.yaml --tags all,dry-run Provisoion the Tavros Cluster with ansible-playbook playbooks/provision_playbook.yaml --inventory <inventory_path>.yaml --tags all","title":"Run Tavros Provision Playbook"},{"location":"playbooks/provision_playbook/","text":"Tavros Provision Playbook \u00b6 Requirements For clusters on AWS IAM Route53 Default Configuration Configuration Variables Kubernetes Cluster Object Kong Object Kong EE Credentials Object Image Registry Object Kong Instance Object Kong Instance EE Object Kuma Object Kuma Mesh Object Kuma mTLS Object Namespace Object Keycloak Object Nexus Object Elastic Cloud Object Gitea Jaeger Table of contents generated with markdown-toc This playbook will use provision a Kubernetes Cluster on AWS. It will then configure the platform components sequentially as configured, and finally commit all resource manifests to Git, for Flux to manage from that point on. The order of component configuration is as follows: * Flux v2 Toolkit - In order to provide platform GitOps * Kubeseal - In order to safely store secrets in GitOps * PostgreSQL - A central configuration database for multiple components * Namespaces - Application namespaced environments * Kuma - Service Meshes * cert-manager - TLS certificate manager for the given domain * Kong - As a API Gateway and Manager, with enterprise edition features if that is enabled * Keycloak - To provide Identity and Access Management, if enabled * Nexus - To provide repository for varios formats, if enabled * Elastic Cloud - To provide observability, if enabled * Gitea - To provide source control, if enabled * Jaeger - To provide observability, if enabled Requirements \u00b6 For clusters on AWS \u00b6 IAM \u00b6 For Tavros cluster on AWS, IAM should be configured as follows: #!/bin/bash aws iam create-group --group-name tavros-provisioner \u200b # required for kops # https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md#setup-iam-user aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name tavros-provisioner aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name tavros-provisioner aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name tavros-provisioner aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name tavros-provisioner aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name tavros-provisioner \u200b aws iam create-user --user-name tavros-ci aws iam add-user-to-group --user-name tavros-ci --group-name tavros-provisioner aws iam create-access-key --user-name tavros-ci The resulting Access Key and Secret should be passed to the Ansible Playbook. For more detailed information see https://kops.sigs.k8s.io/getting_started/aws/ Route53 \u00b6 Route53 should manage the domain to be used for the cluster as a Hosted Zone. It is not necessary for Route53 to serve as the root domain registrar. For more information see Configuring Amazon Route 53 as your DNS service . For more detailed information see https://kops.sigs.k8s.io/getting_started/aws/ Default Configuration \u00b6 There is a default configuration vars file that results in the following: * A Kubernetes cluster on AWS with 3x T2.XLarge masters and 2x T2.Large worker nodes. * A single Keycloak instance with two realms: 'sandbox' and 'prod'. * 2x Kuma meshes: 'sandbox' and 'prod'. * 2x Kong ingress controllers: 'sandbox' and 'prod'. The ingress controllers are part of the Kuma meshes, respectively. * 3x application namespaced environments: 'dev', 'test', and 'prod'. 'dev' and 'test' belong to the 'sandbox' Kuma mesh, and 'prod' to the 'prod' Kuma mesh. To use the default configuration: ansible-playbook playbooks/provision_playbook.yaml \\ --extra-vars '{\"cluster_name\":\"tavros\",\"cluster_domain\":\"example.com\",\"cluster_admin_email\":\"ops@example.com\"}' \\ --inventory \"playbooks/provision_playbook/default_vars.yaml\" Configuration Variables \u00b6 Field Name Type Description Default Value cluster_name String Required The name of the Tavros Kubernetes cluster cluster_domain String Required The domain name to use for the platform. This should be managed by the cloud provider chosen in order to setup routes and certificates cluster_admin_email String Required The email for alerts and general notifications. kubernetes_cluster Kubernetes Cluster Object Required Configuration for the Kubernetes Cluster to be provisioned kong Kong Object Configuration for Kong kuma Kuma Object Configuration for Kuma namespaces [ Namespace Object ] An array of Namespace configurations keycloak Keycloak Object Configuration for Keycloak Kubernetes Cluster Object \u00b6 Field Name Type Description Default Value cloud String The cloud provider to provision the cluster in 'aws' master_count Integer The number of master nodes 3 master_size String The machine instance type for master nodes 'T2.Large' node_count Integer The number of worker nodes 2 node_size String The machine instance type for worker nodes 'T2.XLarge' zones String A comma separated list of availability zones in which to place the machines 'us-east-1,us-east-2' state_bucket String The name of the S3 bucket to place the cluster state in 'troubaodur' ssh_public_key String The SSH public key to setup as authorized user on provisioned machines read from ~/.ssh/id_rsa.pub aws_access_key_id String The AWS IAM AccessKeyId uses aws cli logged in user aws_secret_access_key The AWS IAM SecretAccessKey uses aws cli logged in user keycloak Kubernetes Cluster Keycloak Object Keycloak Config { \"realm\": \"\", \"client_secret\": \"\" } Kubernetes Cluster Keycloak Object \u00b6 Field Name Type Description Default Value realm String Keycloak Realm Name client_secret String Client Secret Kong Object \u00b6 Field Name Type Description Default Value default_ingress_class String The default ingress controller class for other components to use 'prod' ee_creds [ Kong EE Credentials Object ] An array of Kong Enterprise Edition Credentials available for Kong instances to use instances [ Kong Instance Object ] An array of Kong instance object to be configured Kong EE Credentials Object \u00b6 Field Name Type Description Default Value license String Required The Enterprise Edition license name String Required Identifier for EE Kong Instance Kong Instance Object \u00b6 Field Name Type Description Default Value name String Required The name of the Kong instance hybrid Boolean Required Deployment Type ingress_class String Required The name of the ingress class kuma_mesh_name String The name of the Kuma mesh that the Kong instance should be part of ee Kong Instance EE Object The EE configuration for the Kong instance Kong Instance EE Object \u00b6 Field Name Type Description Default Value enabled Boolean Whether to use Enterprise Edition false creds String The name of the Kong EE Credentials resource to use 'default' keycloak Kong Keycloak Object Keycloak Config { \"realm\": \"\", \"client_secret\": \"\" } Kong Keycloak Object \u00b6 Field Name Type Description Default Value realm String Keycloak Realm Name client_secret String Client Secret Kuma Object \u00b6 Field Name Type Description Default Value meshes [ Kuma Mesh Object ] The Kuma mesh configuration object Kuma Mesh Object \u00b6 Field Name Type Description Default Value name String Required The name of the Kuma mesh mtls Kuma mTLS Object The mTLS configuration for the Kuma mesh Kuma mTLS Object \u00b6 Field Name Type Description Default Value enabled Boolean Whether to enable mTLS false Namespace Object \u00b6 Field Name Type Description Default Value name String Required The name of the Namespace kuma_mesh_name String The name of the Kuma mesh this namespace should be a part of Keycloak Object \u00b6 Field Name Type Description Default Value enabled Boolean Whether to use Keycloak true realms Array of Keycloak Realms Object Enumeration of Desired Realms [{\"name\": \"sandbox\"}, {\"name\": \"prod\"}] Keycloak Realms Object \u00b6 Field Name Type Description Default Value name Boolean Whether to use Keycloak Nexus Object \u00b6 Field Name Type Description Default Value enabled Boolean Whether to use Elastic Cloud true keycloak Nexus Keycloak Object Keycloak Config { \"realm\": \"\", \"client_secret\": \"\" } Nexus Keycloak Object \u00b6 Field Name Type Description Default Value realm String Keycloak Realm Name client_secret String Client Secret Elastic Cloud Object \u00b6 Field Name Type Description Default Value enabled Boolean Whether to use Elastic Cloud true ee Elastic Cloud EE Object EE Config { \"enabled\": false } Elastic Cloud EE Object \u00b6 Field Name Type Description Default Value enabled Boolean Whether to use EE trial Boolean Whether to use 30 day cluster trial license licnese String License JSON keycloak Elastic Cloud Keycloak Object Keycloak Config { \"realm\": \"\", \"client_secret\": \"\" } Elastic Cloud Keycloak Object \u00b6 Field Name Type Description Default Value realm String Keycloak Realm Name client_secret String Client Secret Gitea Object \u00b6 Field Name Type Description Default Value enabled Boolean Whether to use Gitea true Jaeger Object \u00b6 Field Name Type Description Default Value enabled Boolean Whether to use Jaeger true keycloak Jaeger Keycloak Object Keycloak Config { \"realm\": \"\", \"client_secret\": \"\" } Jaeger Keycloak Object \u00b6 Field Name Type Description Default Value realm String Keycloak Realm Name client_secret String Client Secret Jenkins Object \u00b6 Field Name Type Description Default Value enabled Boolean Whether to use Jenkins true keycloak Jenkins Keycloak Object Keycloak Config { \"realm\": \"\" } Jenkins Keycloak Object \u00b6 Field Name Type Description Default Value realm String Keycloak Realm Name","title":"Provision"},{"location":"playbooks/provision_playbook/#tavros-provision-playbook","text":"Requirements For clusters on AWS IAM Route53 Default Configuration Configuration Variables Kubernetes Cluster Object Kong Object Kong EE Credentials Object Image Registry Object Kong Instance Object Kong Instance EE Object Kuma Object Kuma Mesh Object Kuma mTLS Object Namespace Object Keycloak Object Nexus Object Elastic Cloud Object Gitea Jaeger Table of contents generated with markdown-toc This playbook will use provision a Kubernetes Cluster on AWS. It will then configure the platform components sequentially as configured, and finally commit all resource manifests to Git, for Flux to manage from that point on. The order of component configuration is as follows: * Flux v2 Toolkit - In order to provide platform GitOps * Kubeseal - In order to safely store secrets in GitOps * PostgreSQL - A central configuration database for multiple components * Namespaces - Application namespaced environments * Kuma - Service Meshes * cert-manager - TLS certificate manager for the given domain * Kong - As a API Gateway and Manager, with enterprise edition features if that is enabled * Keycloak - To provide Identity and Access Management, if enabled * Nexus - To provide repository for varios formats, if enabled * Elastic Cloud - To provide observability, if enabled * Gitea - To provide source control, if enabled * Jaeger - To provide observability, if enabled","title":"Tavros Provision Playbook"},{"location":"playbooks/provision_playbook/#requirements","text":"","title":"Requirements"},{"location":"playbooks/provision_playbook/#for-clusters-on-aws","text":"","title":"For clusters on AWS"},{"location":"playbooks/provision_playbook/#iam","text":"For Tavros cluster on AWS, IAM should be configured as follows: #!/bin/bash aws iam create-group --group-name tavros-provisioner \u200b # required for kops # https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md#setup-iam-user aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name tavros-provisioner aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name tavros-provisioner aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name tavros-provisioner aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name tavros-provisioner aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name tavros-provisioner \u200b aws iam create-user --user-name tavros-ci aws iam add-user-to-group --user-name tavros-ci --group-name tavros-provisioner aws iam create-access-key --user-name tavros-ci The resulting Access Key and Secret should be passed to the Ansible Playbook. For more detailed information see https://kops.sigs.k8s.io/getting_started/aws/","title":"IAM"},{"location":"playbooks/provision_playbook/#route53","text":"Route53 should manage the domain to be used for the cluster as a Hosted Zone. It is not necessary for Route53 to serve as the root domain registrar. For more information see Configuring Amazon Route 53 as your DNS service . For more detailed information see https://kops.sigs.k8s.io/getting_started/aws/","title":"Route53"},{"location":"playbooks/provision_playbook/#default-configuration","text":"There is a default configuration vars file that results in the following: * A Kubernetes cluster on AWS with 3x T2.XLarge masters and 2x T2.Large worker nodes. * A single Keycloak instance with two realms: 'sandbox' and 'prod'. * 2x Kuma meshes: 'sandbox' and 'prod'. * 2x Kong ingress controllers: 'sandbox' and 'prod'. The ingress controllers are part of the Kuma meshes, respectively. * 3x application namespaced environments: 'dev', 'test', and 'prod'. 'dev' and 'test' belong to the 'sandbox' Kuma mesh, and 'prod' to the 'prod' Kuma mesh. To use the default configuration: ansible-playbook playbooks/provision_playbook.yaml \\ --extra-vars '{\"cluster_name\":\"tavros\",\"cluster_domain\":\"example.com\",\"cluster_admin_email\":\"ops@example.com\"}' \\ --inventory \"playbooks/provision_playbook/default_vars.yaml\"","title":"Default Configuration"},{"location":"playbooks/provision_playbook/#configuration-variables","text":"Field Name Type Description Default Value cluster_name String Required The name of the Tavros Kubernetes cluster cluster_domain String Required The domain name to use for the platform. This should be managed by the cloud provider chosen in order to setup routes and certificates cluster_admin_email String Required The email for alerts and general notifications. kubernetes_cluster Kubernetes Cluster Object Required Configuration for the Kubernetes Cluster to be provisioned kong Kong Object Configuration for Kong kuma Kuma Object Configuration for Kuma namespaces [ Namespace Object ] An array of Namespace configurations keycloak Keycloak Object Configuration for Keycloak","title":"Configuration Variables"},{"location":"playbooks/provision_playbook/#kubernetes-cluster-object","text":"Field Name Type Description Default Value cloud String The cloud provider to provision the cluster in 'aws' master_count Integer The number of master nodes 3 master_size String The machine instance type for master nodes 'T2.Large' node_count Integer The number of worker nodes 2 node_size String The machine instance type for worker nodes 'T2.XLarge' zones String A comma separated list of availability zones in which to place the machines 'us-east-1,us-east-2' state_bucket String The name of the S3 bucket to place the cluster state in 'troubaodur' ssh_public_key String The SSH public key to setup as authorized user on provisioned machines read from ~/.ssh/id_rsa.pub aws_access_key_id String The AWS IAM AccessKeyId uses aws cli logged in user aws_secret_access_key The AWS IAM SecretAccessKey uses aws cli logged in user keycloak Kubernetes Cluster Keycloak Object Keycloak Config { \"realm\": \"\", \"client_secret\": \"\" }","title":"Kubernetes Cluster Object"},{"location":"playbooks/provision_playbook/#kubernetes-cluster-keycloak-object","text":"Field Name Type Description Default Value realm String Keycloak Realm Name client_secret String Client Secret","title":"Kubernetes Cluster Keycloak Object"},{"location":"playbooks/provision_playbook/#kong-object","text":"Field Name Type Description Default Value default_ingress_class String The default ingress controller class for other components to use 'prod' ee_creds [ Kong EE Credentials Object ] An array of Kong Enterprise Edition Credentials available for Kong instances to use instances [ Kong Instance Object ] An array of Kong instance object to be configured","title":"Kong Object"},{"location":"playbooks/provision_playbook/#kong-ee-credentials-object","text":"Field Name Type Description Default Value license String Required The Enterprise Edition license name String Required Identifier for EE Kong Instance","title":"Kong EE Credentials Object"},{"location":"playbooks/provision_playbook/#kong-instance-object","text":"Field Name Type Description Default Value name String Required The name of the Kong instance hybrid Boolean Required Deployment Type ingress_class String Required The name of the ingress class kuma_mesh_name String The name of the Kuma mesh that the Kong instance should be part of ee Kong Instance EE Object The EE configuration for the Kong instance","title":"Kong Instance Object"},{"location":"playbooks/provision_playbook/#kong-instance-ee-object","text":"Field Name Type Description Default Value enabled Boolean Whether to use Enterprise Edition false creds String The name of the Kong EE Credentials resource to use 'default' keycloak Kong Keycloak Object Keycloak Config { \"realm\": \"\", \"client_secret\": \"\" }","title":"Kong Instance EE Object"},{"location":"playbooks/provision_playbook/#kong-keycloak-object","text":"Field Name Type Description Default Value realm String Keycloak Realm Name client_secret String Client Secret","title":"Kong Keycloak Object"},{"location":"playbooks/provision_playbook/#kuma-object","text":"Field Name Type Description Default Value meshes [ Kuma Mesh Object ] The Kuma mesh configuration object","title":"Kuma Object"},{"location":"playbooks/provision_playbook/#kuma-mesh-object","text":"Field Name Type Description Default Value name String Required The name of the Kuma mesh mtls Kuma mTLS Object The mTLS configuration for the Kuma mesh","title":"Kuma Mesh Object"},{"location":"playbooks/provision_playbook/#kuma-mtls-object","text":"Field Name Type Description Default Value enabled Boolean Whether to enable mTLS false","title":"Kuma mTLS Object"},{"location":"playbooks/provision_playbook/#namespace-object","text":"Field Name Type Description Default Value name String Required The name of the Namespace kuma_mesh_name String The name of the Kuma mesh this namespace should be a part of","title":"Namespace Object"},{"location":"playbooks/provision_playbook/#keycloak-object","text":"Field Name Type Description Default Value enabled Boolean Whether to use Keycloak true realms Array of Keycloak Realms Object Enumeration of Desired Realms [{\"name\": \"sandbox\"}, {\"name\": \"prod\"}]","title":"Keycloak Object"},{"location":"playbooks/provision_playbook/#keycloak-realms-object","text":"Field Name Type Description Default Value name Boolean Whether to use Keycloak","title":"Keycloak Realms Object"},{"location":"playbooks/provision_playbook/#nexus-object","text":"Field Name Type Description Default Value enabled Boolean Whether to use Elastic Cloud true keycloak Nexus Keycloak Object Keycloak Config { \"realm\": \"\", \"client_secret\": \"\" }","title":"Nexus Object"},{"location":"playbooks/provision_playbook/#nexus-keycloak-object","text":"Field Name Type Description Default Value realm String Keycloak Realm Name client_secret String Client Secret","title":"Nexus Keycloak Object"},{"location":"playbooks/provision_playbook/#elastic-cloud-object","text":"Field Name Type Description Default Value enabled Boolean Whether to use Elastic Cloud true ee Elastic Cloud EE Object EE Config { \"enabled\": false }","title":"Elastic Cloud Object"},{"location":"playbooks/provision_playbook/#elastic-cloud-ee-object","text":"Field Name Type Description Default Value enabled Boolean Whether to use EE trial Boolean Whether to use 30 day cluster trial license licnese String License JSON keycloak Elastic Cloud Keycloak Object Keycloak Config { \"realm\": \"\", \"client_secret\": \"\" }","title":"Elastic Cloud EE Object"},{"location":"playbooks/provision_playbook/#elastic-cloud-keycloak-object","text":"Field Name Type Description Default Value realm String Keycloak Realm Name client_secret String Client Secret","title":"Elastic Cloud Keycloak Object"},{"location":"playbooks/provision_playbook/#gitea-object","text":"Field Name Type Description Default Value enabled Boolean Whether to use Gitea true","title":"Gitea Object"},{"location":"playbooks/provision_playbook/#jaeger-object","text":"Field Name Type Description Default Value enabled Boolean Whether to use Jaeger true keycloak Jaeger Keycloak Object Keycloak Config { \"realm\": \"\", \"client_secret\": \"\" }","title":"Jaeger Object"},{"location":"playbooks/provision_playbook/#jaeger-keycloak-object","text":"Field Name Type Description Default Value realm String Keycloak Realm Name client_secret String Client Secret","title":"Jaeger Keycloak Object"},{"location":"playbooks/provision_playbook/#jenkins-object","text":"Field Name Type Description Default Value enabled Boolean Whether to use Jenkins true keycloak Jenkins Keycloak Object Keycloak Config { \"realm\": \"\" }","title":"Jenkins Object"},{"location":"playbooks/provision_playbook/#jenkins-keycloak-object","text":"Field Name Type Description Default Value realm String Keycloak Realm Name","title":"Jenkins Keycloak Object"},{"location":"reference/","text":"","title":"Reference"},{"location":"releases/0.1-NOTES/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Unreleased \u00b6 1.0.0 - 2017-06-20 \u00b6 Added \u00b6 New visual identity by @tylerfortune8 . Version navigation. Links to latest released version in previous versions. \"Why keep a changelog?\" section. \"Who needs a changelog?\" section. \"How do I make a changelog?\" section. \"Frequently Asked Questions\" section. New \"Guiding Principles\" sub-section to \"How do I make a changelog?\". Simplified and Traditional Chinese translations from @tianshuo . German translation from @mpbzh & @Art4 . Italian translation from @azkidenz . Swedish translation from @magol . Turkish translation from @karalamalar . French translation from @zapashcanon . Brazilian Portugese translation from @Webysther . Polish translation from @amielucha & @m-aciek . Russian translation from @aishek . Czech translation from @h4vry . Slovak translation from @jkostolansky . Korean translation from @pierceh89 . Croatian translation from @porx . Persian translation from @Hameds . Ukrainian translation from @osadchyi-s . Changed \u00b6 Start using \"changelog\" over \"change log\" since it's the common usage. Start versioning based on the current English version at 0.3.0 to help translation authors keep things up-to-date. Rewrite \"What makes unicorns cry?\" section. Rewrite \"Ignoring Deprecations\" sub-section to clarify the ideal scenario. Improve \"Commit log diffs\" sub-section to further argument against them. Merge \"Why can\u2019t people just use a git log diff?\" with \"Commit log diffs\" Fix typos in Simplified Chinese and Traditional Chinese translations. Fix typos in Brazilian Portuguese translation. Fix typos in Turkish translation. Fix typos in Czech translation. Fix typos in Swedish translation. Improve phrasing in French translation. Fix phrasing and spelling in German translation. Removed \u00b6 Section about \"changelog\" vs \"CHANGELOG\". 0.3.0 - 2015-12-03 \u00b6 Added \u00b6 RU translation from @aishek . pt-BR translation from @tallesl . es-ES translation from @ZeliosAriex . 0.2.0 - 2015-10-06 \u00b6 Changed \u00b6 Remove exclusionary mentions of \"open source\" since this project can benefit both \"open\" and \"closed\" source projects equally. 0.1.0 - 2015-10-06 \u00b6 Added \u00b6 Answer \"Should you ever rewrite a change log?\". Changed \u00b6 Improve argument against commit logs. Start following SemVer properly. 0.0.8 - 2015-02-17 \u00b6 Changed \u00b6 Update year to match in every README example. Reluctantly stop making fun of Brits only, since most of the world writes dates in a strange way. Fixed \u00b6 Fix typos in recent README changes. Update outdated unreleased diff link. 0.0.7 - 2015-02-16 \u00b6 Added \u00b6 Link, and make it obvious that date format is ISO 8601. Changed \u00b6 Clarified the section on \"Is there a standard change log format?\". Fixed \u00b6 Fix Markdown links to tag comparison URL with footnote-style links. 0.0.6 - 2014-12-12 \u00b6 Added \u00b6 README section on \"yanked\" releases. 0.0.5 - 2014-08-09 \u00b6 Added \u00b6 Markdown links to version tags on release headings. Unreleased section to gather unreleased changes and encourage note keeping prior to releases. 0.0.4 - 2014-08-09 \u00b6 Added \u00b6 Better explanation of the difference between the file (\"CHANGELOG\") and its function \"the change log\". Changed \u00b6 Refer to a \"change log\" instead of a \"CHANGELOG\" throughout the site to differentiate between the file and the purpose of the file \u2014 the logging of changes. Removed \u00b6 Remove empty sections from CHANGELOG, they occupy too much space and create too much noise in the file. People will have to assume that the missing sections were intentionally left out because they contained no notable changes. 0.0.3 - 2014-08-09 \u00b6 Added \u00b6 \"Why should I care?\" section mentioning The Changelog podcast. 0.0.2 - 2014-07-10 \u00b6 Added \u00b6 Explanation of the recommended reverse chronological release ordering. 0.0.1 - 2014-05-31 \u00b6 Added \u00b6 This CHANGELOG file to hopefully serve as an evolving example of a standardized open source project CHANGELOG. CNAME file to enable GitHub Pages custom domain README now contains answers to common questions about CHANGELOGs Good examples and basic guidelines, including proper date formatting. Counter-examples: \"What makes unicorns cry?\"","title":"0.1"},{"location":"releases/0.1-NOTES/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"releases/0.1-NOTES/#unreleased","text":"","title":"Unreleased"},{"location":"releases/0.1-NOTES/#100-2017-06-20","text":"","title":"1.0.0 - 2017-06-20"},{"location":"releases/0.1-NOTES/#added","text":"New visual identity by @tylerfortune8 . Version navigation. Links to latest released version in previous versions. \"Why keep a changelog?\" section. \"Who needs a changelog?\" section. \"How do I make a changelog?\" section. \"Frequently Asked Questions\" section. New \"Guiding Principles\" sub-section to \"How do I make a changelog?\". Simplified and Traditional Chinese translations from @tianshuo . German translation from @mpbzh & @Art4 . Italian translation from @azkidenz . Swedish translation from @magol . Turkish translation from @karalamalar . French translation from @zapashcanon . Brazilian Portugese translation from @Webysther . Polish translation from @amielucha & @m-aciek . Russian translation from @aishek . Czech translation from @h4vry . Slovak translation from @jkostolansky . Korean translation from @pierceh89 . Croatian translation from @porx . Persian translation from @Hameds . Ukrainian translation from @osadchyi-s .","title":"Added"},{"location":"releases/0.1-NOTES/#changed","text":"Start using \"changelog\" over \"change log\" since it's the common usage. Start versioning based on the current English version at 0.3.0 to help translation authors keep things up-to-date. Rewrite \"What makes unicorns cry?\" section. Rewrite \"Ignoring Deprecations\" sub-section to clarify the ideal scenario. Improve \"Commit log diffs\" sub-section to further argument against them. Merge \"Why can\u2019t people just use a git log diff?\" with \"Commit log diffs\" Fix typos in Simplified Chinese and Traditional Chinese translations. Fix typos in Brazilian Portuguese translation. Fix typos in Turkish translation. Fix typos in Czech translation. Fix typos in Swedish translation. Improve phrasing in French translation. Fix phrasing and spelling in German translation.","title":"Changed"},{"location":"releases/0.1-NOTES/#removed","text":"Section about \"changelog\" vs \"CHANGELOG\".","title":"Removed"},{"location":"releases/0.1-NOTES/#030-2015-12-03","text":"","title":"0.3.0 - 2015-12-03"},{"location":"releases/0.1-NOTES/#added_1","text":"RU translation from @aishek . pt-BR translation from @tallesl . es-ES translation from @ZeliosAriex .","title":"Added"},{"location":"releases/0.1-NOTES/#020-2015-10-06","text":"","title":"0.2.0 - 2015-10-06"},{"location":"releases/0.1-NOTES/#changed_1","text":"Remove exclusionary mentions of \"open source\" since this project can benefit both \"open\" and \"closed\" source projects equally.","title":"Changed"},{"location":"releases/0.1-NOTES/#010-2015-10-06","text":"","title":"0.1.0 - 2015-10-06"},{"location":"releases/0.1-NOTES/#added_2","text":"Answer \"Should you ever rewrite a change log?\".","title":"Added"},{"location":"releases/0.1-NOTES/#changed_2","text":"Improve argument against commit logs. Start following SemVer properly.","title":"Changed"},{"location":"releases/0.1-NOTES/#008-2015-02-17","text":"","title":"0.0.8 - 2015-02-17"},{"location":"releases/0.1-NOTES/#changed_3","text":"Update year to match in every README example. Reluctantly stop making fun of Brits only, since most of the world writes dates in a strange way.","title":"Changed"},{"location":"releases/0.1-NOTES/#fixed","text":"Fix typos in recent README changes. Update outdated unreleased diff link.","title":"Fixed"},{"location":"releases/0.1-NOTES/#007-2015-02-16","text":"","title":"0.0.7 - 2015-02-16"},{"location":"releases/0.1-NOTES/#added_3","text":"Link, and make it obvious that date format is ISO 8601.","title":"Added"},{"location":"releases/0.1-NOTES/#changed_4","text":"Clarified the section on \"Is there a standard change log format?\".","title":"Changed"},{"location":"releases/0.1-NOTES/#fixed_1","text":"Fix Markdown links to tag comparison URL with footnote-style links.","title":"Fixed"},{"location":"releases/0.1-NOTES/#006-2014-12-12","text":"","title":"0.0.6 - 2014-12-12"},{"location":"releases/0.1-NOTES/#added_4","text":"README section on \"yanked\" releases.","title":"Added"},{"location":"releases/0.1-NOTES/#005-2014-08-09","text":"","title":"0.0.5 - 2014-08-09"},{"location":"releases/0.1-NOTES/#added_5","text":"Markdown links to version tags on release headings. Unreleased section to gather unreleased changes and encourage note keeping prior to releases.","title":"Added"},{"location":"releases/0.1-NOTES/#004-2014-08-09","text":"","title":"0.0.4 - 2014-08-09"},{"location":"releases/0.1-NOTES/#added_6","text":"Better explanation of the difference between the file (\"CHANGELOG\") and its function \"the change log\".","title":"Added"},{"location":"releases/0.1-NOTES/#changed_5","text":"Refer to a \"change log\" instead of a \"CHANGELOG\" throughout the site to differentiate between the file and the purpose of the file \u2014 the logging of changes.","title":"Changed"},{"location":"releases/0.1-NOTES/#removed_1","text":"Remove empty sections from CHANGELOG, they occupy too much space and create too much noise in the file. People will have to assume that the missing sections were intentionally left out because they contained no notable changes.","title":"Removed"},{"location":"releases/0.1-NOTES/#003-2014-08-09","text":"","title":"0.0.3 - 2014-08-09"},{"location":"releases/0.1-NOTES/#added_7","text":"\"Why should I care?\" section mentioning The Changelog podcast.","title":"Added"},{"location":"releases/0.1-NOTES/#002-2014-07-10","text":"","title":"0.0.2 - 2014-07-10"},{"location":"releases/0.1-NOTES/#added_8","text":"Explanation of the recommended reverse chronological release ordering.","title":"Added"},{"location":"releases/0.1-NOTES/#001-2014-05-31","text":"","title":"0.0.1 - 2014-05-31"},{"location":"releases/0.1-NOTES/#added_9","text":"This CHANGELOG file to hopefully serve as an evolving example of a standardized open source project CHANGELOG. CNAME file to enable GitHub Pages custom domain README now contains answers to common questions about CHANGELOGs Good examples and basic guidelines, including proper date formatting. Counter-examples: \"What makes unicorns cry?\"","title":"Added"},{"location":"roles/aks/","text":"","title":"Overview"},{"location":"roles/aks/config/","text":"","title":"Prereqs"},{"location":"welcome/concepts/","text":"","title":"Concepts"},{"location":"welcome/contribute/","text":"","title":"Contribute"}]}